{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAeNBJIH4eo7"
      },
      "source": [
        "Import des bibliothèque necessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXJFT3hINTdG",
        "outputId": "89c43878-8a21-4c35-c261-f17bcd1c8079"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD, NMF\n",
        "from itertools import product\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc4hk45n4kjt"
      },
      "source": [
        "Class Data pour tous les pré-traitement, vectorisation et réduction de la matrice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-JQ20_eh4J4b"
      },
      "outputs": [],
      "source": [
        "class Data:\n",
        "    def __init__(self, limit=5000,language=None,ignore_stopwords=None,mode=None,overwrite=None,):\n",
        "        self.data_brut = pd.read_json(\"train.jsonl\", lines=True)[[\"texte_annonce\", \"cal_réponse_signalement\"]]\n",
        "        self.data = self.data_brut.iloc[:limit]\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.data\n",
        "\n",
        "    def rien(self, x):\n",
        "        pass\n",
        "\n",
        "    def racinisation(self, text_column):\n",
        "        stemmer = SnowballStemmer(\"french\")\n",
        "        self.data.loc[:, text_column] = self.data[text_column].apply(lambda x: ' '.join([stemmer.stem(word) for word in str(x).split()]))\n",
        "\n",
        "    def lemmatisation(self, text_column):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        self.data.loc[:, text_column] = self.data[text_column].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in str(x).split()]))\n",
        "\n",
        "    def vectorisation_simple(self, text_column):\n",
        "        vectorizer = CountVectorizer()\n",
        "        vect_data = vectorizer.fit_transform(self.data[text_column])\n",
        "        vect_df = pd.DataFrame(vect_data.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "        self.data = pd.concat([vect_df, self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def vectorisation_ponderee(self, text_column):\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        vect_data = vectorizer.fit_transform(self.data[text_column])\n",
        "        vect_df = pd.DataFrame(vect_data.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "        self.data = pd.concat([vect_df, self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def reduction_svd(self, n_components=100):\n",
        "        svd = TruncatedSVD(n_components=n_components)\n",
        "        svd_result = svd.fit_transform(self.data.iloc[:, :-1])\n",
        "        self.data = pd.concat([pd.DataFrame(svd_result), self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def reduction_nmf(self, n_components=100):\n",
        "        nmf = NMF(n_components=n_components)\n",
        "        nmf_result = nmf.fit_transform(self.data.iloc[:, :-1])\n",
        "        self.data = pd.concat([pd.DataFrame(nmf_result), self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def process_and_export(self, text_column=\"texte_annonce\", n_components=100):\n",
        "        # Définir toutes les combinaisons possibles\n",
        "        preprocessing_methods = [self.rien, self.racinisation, self.lemmatisation]\n",
        "        vectorisation_methods = [self.vectorisation_simple, self.vectorisation_ponderee]\n",
        "        reduction_methods = [self.reduction_svd, self.reduction_nmf]\n",
        "\n",
        "        combinations = product(preprocessing_methods, vectorisation_methods, reduction_methods)\n",
        "\n",
        "        # Boucler sur toutes les combinaisons\n",
        "        for i, (preprocess, vectorize, reduce) in enumerate(combinations):\n",
        "            # Réinitialiser les données à l'état brut\n",
        "            self.data = self.data_brut.copy()\n",
        "\n",
        "            # Appliquer les méthodes\n",
        "            preprocess(text_column)\n",
        "            vectorize(text_column)\n",
        "            reduce(n_components)\n",
        "\n",
        "            # Exporter le résultat\n",
        "            output_file = f\"output_combination_{i+1}.jsonl\"\n",
        "            self.data.to_json(output_file, orient=\"records\", lines=True)\n",
        "            print(f\"Exporté : {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFjpIvyd_Y5S"
      },
      "source": [
        "Class global des classifieurs permettant de faire de l'héritage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RA_Ne8kbpOGD"
      },
      "outputs": [],
      "source": [
        "class Classifieur:\n",
        "    def __init__(self, data):\n",
        "        # Chargement et découpage des données\n",
        "        self.chargement(data)\n",
        "        self.decoupage()\n",
        "\n",
        "    def chargement(self, data):\n",
        "        # Chargement des données et séparation X, y\n",
        "        self.data = data\n",
        "        self.X = data.iloc[:, :-1]  # Toutes les colonnes sauf la dernière (X)\n",
        "        self.y = data.iloc[:, -1]   # Dernière colonne (y)\n",
        "\n",
        "    def decoupage(self):\n",
        "        # Découpage en jeu d'entrainement et test\n",
        "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.X, self.y, test_size=0.6, random_state=0)\n",
        "\n",
        "    def entrainement(self):\n",
        "        pass\n",
        "\n",
        "    def rien(self):\n",
        "        pass\n",
        "\n",
        "    def taux_reussite(self):\n",
        "        # Calcul du taux de réussite\n",
        "        y_pred = self.classifier.predict(self.X_test)\n",
        "        accuracy = accuracy_score(self.Y_test, y_pred)\n",
        "        return f\"{accuracy:.3f}\"\n",
        "\n",
        "    def f1_score(self):\n",
        "        # Calcul du F1-score\n",
        "        from sklearn.metrics import f1_score\n",
        "        y_pred = self.classifier.predict(self.X_test)\n",
        "        f1 = f1_score(self.Y_test, y_pred, average='weighted')\n",
        "        return f\"{f1:.3f}\"\n",
        "\n",
        "    def recherche_hyperparametres(self,param_grid):\n",
        "          # Recherche des meilleurs hyperparamètres avec GridSearchCV\n",
        "          grid_search = GridSearchCV(\n",
        "              self.classifier,\n",
        "              param_grid,\n",
        "              cv=5,  # Validation croisée 5-fold\n",
        "              scoring='f1',\n",
        "          )\n",
        "          grid_search.fit(self.X_train, self.Y_train)\n",
        "          self.classifier = grid_search.best_estimator_\n",
        "          print(\"Meilleurs hyperparamètres :\", grid_search.best_params_)\n",
        "          return grid_search.best_params_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nAx2MGzSOn_S"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "\n",
        "class Multinomial(Classifieur):\n",
        "    def __init__(self, data, alpha = None, fit_prior = None):\n",
        "        super().__init__(data)\n",
        "        self.classifier = MultinomialNB()\n",
        "        if alpha : self.classifier.alpha = alpha\n",
        "        if fit_prior : self.classifier.fit_prior = fit_prior\n",
        "\n",
        "    def entrainement(self):\n",
        "        # Entraînement du modèle avec les données\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Mbtkcyg7njLK"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "\n",
        "class Bernoulli(Classifieur):\n",
        "    def __init__(self, data, alpha = None, fit_prior = None, binarize = None):\n",
        "        super().__init__(data)\n",
        "\n",
        "        self.classifier = BernoulliNB()\n",
        "        if alpha : self.classifier.alpha = alpha\n",
        "        if fit_prior : self.classifier.fit_prior = fit_prior\n",
        "        if binarize : self.classifier.binarize = binarize\n",
        "\n",
        "\n",
        "    def entrainement(self):\n",
        "        # Entraînement du modèle avec les données\n",
        "        self.classifier.fit(self.X_train, self.Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5Kpxa-J6njnS"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "class Gaussian(Classifieur):\n",
        "    def __init__(self, data, var_smoothing = None):\n",
        "        super().__init__(data)\n",
        "        self.classifier = GaussianNB()\n",
        "        if var_smoothing : self.classifier.var_smoothing = var_smoothing\n",
        "\n",
        "    def entrainement(self):\n",
        "        # Entraînement du modèle avec les données\n",
        "        self.classifier.fit(self.X_train, self.Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8fqw-IaPRTQ0"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "class RegressionLogistique(Classifieur):\n",
        "    def __init__(self, data, penalty = None, C = None, solver = None, max_iter = None, l1_ratio = None, random_state = None):\n",
        "        super().__init__(data)\n",
        "        self.classifier = LogisticRegression()\n",
        "        if penalty : self.classifier.penalty = penalty\n",
        "        if C : self.classifier.C = C\n",
        "        if solver : self.classifier.solver = solver\n",
        "        if max_iter : self.classifier.max_iter = max_iter\n",
        "        if l1_ratio : self.classifier.l1_ratio\n",
        "        if random_state : self.classifier.random_state = random_state\n",
        "\n",
        "    def entrainement(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "pE4gII77RjTM"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "class KPlusProchesVoisins(Classifieur):\n",
        "    def __init__(self, data, n_neighbors = 3, weights = None, algorithm = None, p = None):\n",
        "        super().__init__(data)\n",
        "\n",
        "        self.classifier = KNeighborsClassifier()\n",
        "        if n_neighbors : self.classifier.n_neighbors = n_neighbors\n",
        "        if weights : self.classifier.weights = weights\n",
        "        if algorithm : self.classifier.algorithm = algorithm\n",
        "        if p : self.classifier.p = p\n",
        "\n",
        "    def entrainement(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RPUV2vAXRx4G"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "class ArbreDeDecision(Classifieur):\n",
        "    def __init__(self, data, criterion = None, max_depth = None, min_samples_split = None, min_samples_leaf = None, max_features = None, ccp_alpha = None, random_state = None):\n",
        "        super().__init__(data)\n",
        "        self.classifier = DecisionTreeClassifier()\n",
        "        if criterion : self.classifier.criterion = criterion\n",
        "        if max_depth : self.classifier.max_depth = max_depth\n",
        "        if min_samples_split : self.classifier.min_samples_split = min_samples_split\n",
        "        if min_samples_leaf : self.classifier.min_samples_leaf = min_samples_leaf\n",
        "        if max_features : self.classifier.max_features = max_features\n",
        "        if ccp_alpha : self.classifier.ccp_alpha = ccp_alpha\n",
        "        if random_state : self.classifier.random_state = random_state\n",
        "\n",
        "    def entrainement(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "t-mIMWs9R_AW"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "class ForetAleatoire(Classifieur):\n",
        "    def __init__(self, data, n_estimators = 100, criterion = None, max_depth = None, min_samples_split = None, min_samples_leaf = None, max_features = None, bootstrap = None, ccp_alpha = None, random_state = 0 ):\n",
        "        super().__init__(data)\n",
        "        self.classifier = RandomForestClassifier()\n",
        "        if n_estimators : self.classifier.n_estimators = n_estimators\n",
        "        if criterion : self.classifier.criterion = criterion\n",
        "        if max_depth : self.classifier.max_depth = max_depth\n",
        "        if min_samples_split : self.classifier.min_samples_split = min_samples_split\n",
        "        if min_samples_leaf : self.classifier.min_samples_leaf = min_samples_leaf\n",
        "        if max_features : self.classifier.max_features = max_features\n",
        "        if bootstrap : self.classifier.bootstrap = bootstrap\n",
        "        if ccp_alpha : self.classifier.ccp_alpha = ccp_alpha\n",
        "        if random_state : self.classifier.random_state = random_state\n",
        "\n",
        "    def entrainement(self, n_estimators=100):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeF-Q305_nzb"
      },
      "source": [
        "Fonction ultra débile qui teste tous les classifieurs, pré traitements, vectorisation et réduction afin d'avoir le meilleurs taux de réussite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HHJ6FYyH4au6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('lemmatisation', 'vectorisation_simple', 'svd', 'KPlusProchesVoisins', '0.631')\n",
            "('lemmatisation', 'vectorisation_simple', 'svd', 'ArbreDeDecision', '0.621')\n",
            "('lemmatisation', 'vectorisation_simple', 'svd', 'ForetAleatoire', '0.626')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\d.truong\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1770: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('lemmatisation', 'vectorisation_ponderee', 'nmf', 'KPlusProchesVoisins', '0.651')\n",
            "('lemmatisation', 'vectorisation_ponderee', 'nmf', 'ArbreDeDecision', '0.643')\n",
            "('lemmatisation', 'vectorisation_ponderee', 'nmf', 'ForetAleatoire', '0.662')\n",
            "('racinisation', 'vectorisation_simple', 'svd', 'KPlusProchesVoisins', '0.635')\n",
            "('racinisation', 'vectorisation_simple', 'svd', 'ArbreDeDecision', '0.607')\n",
            "('racinisation', 'vectorisation_simple', 'svd', 'ForetAleatoire', '0.624')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\d.truong\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1770: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('racinisation', 'vectorisation_ponderee', 'nmf', 'KPlusProchesVoisins', '0.645')\n",
            "('racinisation', 'vectorisation_ponderee', 'nmf', 'ArbreDeDecision', '0.631')\n",
            "('racinisation', 'vectorisation_ponderee', 'nmf', 'ForetAleatoire', '0.652')\n",
            "('rien', 'vectorisation_simple', 'svd', 'KPlusProchesVoisins', '0.630')\n",
            "('rien', 'vectorisation_simple', 'svd', 'ArbreDeDecision', '0.603')\n",
            "('rien', 'vectorisation_simple', 'svd', 'ForetAleatoire', '0.630')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\d.truong\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1770: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('rien', 'vectorisation_ponderee', 'nmf', 'KPlusProchesVoisins', '0.650')\n",
            "('rien', 'vectorisation_ponderee', 'nmf', 'ArbreDeDecision', '0.646')\n",
            "('rien', 'vectorisation_ponderee', 'nmf', 'ForetAleatoire', '0.650')\n",
            "lemmatisation - vectorisation_ponderee - nmf - ForetAleatoire : 0.662\n",
            "racinisation - vectorisation_ponderee - nmf - ForetAleatoire : 0.652\n",
            "lemmatisation - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.651\n",
            "rien - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.650\n",
            "rien - vectorisation_ponderee - nmf - ForetAleatoire : 0.650\n",
            "rien - vectorisation_ponderee - nmf - ArbreDeDecision : 0.646\n",
            "racinisation - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.645\n",
            "lemmatisation - vectorisation_ponderee - nmf - ArbreDeDecision : 0.643\n",
            "racinisation - vectorisation_simple - svd - KPlusProchesVoisins : 0.635\n",
            "lemmatisation - vectorisation_simple - svd - KPlusProchesVoisins : 0.631\n",
            "racinisation - vectorisation_ponderee - nmf - ArbreDeDecision : 0.631\n",
            "rien - vectorisation_simple - svd - KPlusProchesVoisins : 0.630\n",
            "rien - vectorisation_simple - svd - ForetAleatoire : 0.630\n",
            "lemmatisation - vectorisation_simple - svd - ForetAleatoire : 0.626\n",
            "racinisation - vectorisation_simple - svd - ForetAleatoire : 0.624\n",
            "lemmatisation - vectorisation_simple - svd - ArbreDeDecision : 0.621\n",
            "racinisation - vectorisation_simple - svd - ArbreDeDecision : 0.607\n",
            "rien - vectorisation_simple - svd - ArbreDeDecision : 0.603\n"
          ]
        }
      ],
      "source": [
        "def possibilite():\n",
        "    results = []  # Liste pour stocker les résultats\n",
        "\n",
        "    for x in [\"lemmatisation\", \"racinisation\",\"rien\"]:\n",
        "        for y in [\"vectorisation_simple\", \"vectorisation_ponderee\"]:\n",
        "            data = Data()\n",
        "            getattr(data, x)(\"texte_annonce\")\n",
        "            getattr(data, y)(\"texte_annonce\")\n",
        "\n",
        "            if y == \"vectorisation_ponderee\":\n",
        "                data.reduction_nmf()\n",
        "                z = \"nmf\"\n",
        "            else:\n",
        "                data.reduction_svd()\n",
        "                z = \"svd\"\n",
        "\n",
        "            # Parcours des classifieurs\n",
        "            for i in [ KPlusProchesVoisins, ArbreDeDecision, ForetAleatoire]:\n",
        "                classifieur = i(data.get_data())\n",
        "                classifieur.entrainement()\n",
        "                f1_score = classifieur.f1_score()\n",
        "                print((x, y, z, i.__name__,f1_score))\n",
        "\n",
        "                # Ajout des résultats à la liste sous forme de tuple\n",
        "                results.append((x, y, z, i.__name__,f1_score))\n",
        "\n",
        "    # Tri des résultats par taux de réussite (du plus élevé au plus faible)\n",
        "    results_sorted = sorted(results, key=lambda x: x[4], reverse=True)\n",
        "\n",
        "    # Affichage des résultats triés\n",
        "    for res in results_sorted:\n",
        "        print(f\"{res[0]} - {res[1]} - {res[2]} - {res[3]} : {res[4]}\")\n",
        "\n",
        "possibilite()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKATkwh7AhPV"
      },
      "source": [
        "Résultat lancé en amont (tps: 16 min)\n",
        "\n",
        "- lemmatisation - vectorisation_ponderee - nmf - ForetAleatoire : 0.741\n",
        "- racinisation - vectorisation_ponderee - nmf - ForetAleatoire : 0.738\n",
        "- rien - vectorisation_ponderee - nmf - ForetAleatoire : 0.737\n",
        "- lemmatisation - vectorisation_ponderee - nmf - RegressionLogistique : 0.734\n",
        "- racinisation - vectorisation_simple - svd - ForetAleatoire : 0.734\n",
        "- racinisation - vectorisation_ponderee - nmf - RegressionLogistique : 0.734\n",
        "- rien - vectorisation_ponderee - nmf - RegressionLogistique : 0.734\n",
        "- lemmatisation - vectorisation_simple - svd - ForetAleatoire : 0.733\n",
        "- rien - vectorisation_simple - svd - ForetAleatoire : 0.732\n",
        "- lemmatisation - vectorisation_simple - svd - RegressionLogistique : 0.725\n",
        "- racinisation - vectorisation_simple - svd - RegressionLogistique : 0.723\n",
        "- rien - vectorisation_simple - svd - RegressionLogistique : 0.723\n",
        "- rien - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.667\n",
        "- lemmatisation - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.658\n",
        "- racinisation - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.657\n",
        "- racinisation - vectorisation_simple - svd - KPlusProchesVoisins : 0.651\n",
        "- lemmatisation - vectorisation_simple - svd - KPlusProchesVoisins : 0.648\n",
        "- rien - vectorisation_simple - svd - KPlusProchesVoisins : 0.644\n",
        "- lemmatisation - vectorisation_ponderee - nmf - ArbreDeDecision : 0.643\n",
        "- racinisation - vectorisation_ponderee - nmf - ArbreDeDecision : 0.640\n",
        "- rien - vectorisation_ponderee - nmf - ArbreDeDecision : 0.637\n",
        "- lemmatisation - vectorisation_simple - svd - ArbreDeDecision : 0.623\n",
        "- rien - vectorisation_simple - svd - ArbreDeDecision : 0.617\n",
        "- racinisation - vectorisation_simple - svd - ArbreDeDecision : 0.586\n",
        "- rien - vectorisation_ponderee - nmf - BayesienNaif : 0.494\n",
        "- lemmatisation - vectorisation_ponderee - nmf - BayesienNaif : 0.491\n",
        "- racinisation - vectorisation_ponderee - nmf - BayesienNaif : 0.483\n",
        "- lemmatisation - vectorisation_simple - svd - BayesienNaif : 0.329\n",
        "- rien - vectorisation_simple - svd - BayesienNaif : 0.328\n",
        "- racinisation - vectorisation_simple - svd - BayesienNaif : 0.326\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "lemmatisation - vectorisation_ponderee - nmf - ForetAleatoire : 0.657\n",
        "rien - vectorisation_ponderee - nmf - ForetAleatoire : 0.656\n",
        "rien - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.655\n",
        "racinisation - vectorisation_ponderee - nmf - ForetAleatoire : 0.649\n",
        "rien - vectorisation_simple - svd - RegressionLogistique : 0.649\n",
        "lemmatisation - vectorisation_simple - svd - RegressionLogistique : 0.648\n",
        "lemmatisation - vectorisation_ponderee - nmf - ArbreDeDecision : 0.647\n",
        "racinisation - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.647\n",
        "racinisation - vectorisation_simple - svd - RegressionLogistique : 0.644\n",
        "racinisation - vectorisation_simple - svd - KPlusProchesVoisins : 0.639\n",
        "lemmatisation - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.638\n",
        "rien - vectorisation_ponderee - nmf - ArbreDeDecision : 0.638\n",
        "racinisation - vectorisation_ponderee - nmf - ArbreDeDecision : 0.637\n",
        "lemmatisation - vectorisation_simple - svd - KPlusProchesVoisins : 0.631\n",
        "rien - vectorisation_simple - svd - KPlusProchesVoisins : 0.630\n",
        "racinisation - vectorisation_simple - svd - ForetAleatoire : 0.627\n",
        "rien - vectorisation_simple - svd - ForetAleatoire : 0.626\n",
        "lemmatisation - vectorisation_simple - svd - ForetAleatoire : 0.624\n",
        "lemmatisation - vectorisation_ponderee - nmf - RegressionLogistique : 0.621\n",
        "racinisation - vectorisation_ponderee - nmf - RegressionLogistique : 0.621\n",
        "rien - vectorisation_ponderee - nmf - RegressionLogistique : 0.621\n",
        "rien - vectorisation_simple - svd - ArbreDeDecision : 0.612\n",
        "racinisation - vectorisation_simple - svd - ArbreDeDecision : 0.610\n",
        "lemmatisation - vectorisation_simple - svd - ArbreDeDecision : 0.594"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "lemmatisation - vectorisation_ponderee - nmf - ForetAleatoire : 0.662\n",
        "racinisation - vectorisation_ponderee - nmf - ForetAleatoire : 0.652\n",
        "lemmatisation - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.651\n",
        "rien - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.650\n",
        "rien - vectorisation_ponderee - nmf - ForetAleatoire : 0.650\n",
        "rien - vectorisation_ponderee - nmf - ArbreDeDecision : 0.646\n",
        "racinisation - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.645\n",
        "lemmatisation - vectorisation_ponderee - nmf - ArbreDeDecision : 0.643\n",
        "racinisation - vectorisation_simple - svd - KPlusProchesVoisins : 0.635\n",
        "lemmatisation - vectorisation_simple - svd - KPlusProchesVoisins : 0.631\n",
        "racinisation - vectorisation_ponderee - nmf - ArbreDeDecision : 0.631\n",
        "rien - vectorisation_simple - svd - KPlusProchesVoisins : 0.630\n",
        "rien - vectorisation_simple - svd - ForetAleatoire : 0.630\n",
        "lemmatisation - vectorisation_simple - svd - ForetAleatoire : 0.626\n",
        "racinisation - vectorisation_simple - svd - ForetAleatoire : 0.624\n",
        "lemmatisation - vectorisation_simple - svd - ArbreDeDecision : 0.621\n",
        "racinisation - vectorisation_simple - svd - ArbreDeDecision : 0.607\n",
        "rien - vectorisation_simple - svd - ArbreDeDecision : 0.603"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (4089700616.py, line 9)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    results.append((if.__name__, f1_score))\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "results=[]\n",
        "\n",
        "def boucler_classeur(data, grid):\n",
        "    for i in [Multinomial, Gaussian, Bernoulli,  RegressionLogistique, KPlusProchesVoisins, ArbreDeDecision, ForetAleatoire]:\n",
        "        classifieur = i(data)\n",
        "        classifieur= i(data,classifieur.recherche_hyperparametres(grid))\n",
        "        f1_score = classifieur.f1_score()\n",
        "\n",
        "        results.append((i.__name__, f1_score))\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texte nettoyé : les articles comme le la les sont fréquents fréquents mais inutiles pour notre analyse de une\n",
            "Tokens après suppression des mots vides : ['articles', 'comme', 'fréquents', 'fréquents', 'inutiles', 'analyse']\n",
            "Tokens après lemmatisation : ['article', 'comme', 'fréquents', 'fréquents', 'inutiles', 'analyse']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Téléchargements nécessaires pour NLTK\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "\n",
        "# Exemple de texte\n",
        "texte = \"Les articles comme 'le', 'la', 'les' sont fréquents fréquents mais inutiles pour notre analyse de une.\"\n",
        "\n",
        "# 1. Nettoyage de base\n",
        "def nettoyer_texte(texte):\n",
        "    # Convertir en minuscules\n",
        "    texte = texte.lower()\n",
        "    # Supprimer la ponctuation\n",
        "    texte = texte.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Supprimer les chiffres\n",
        "    texte = ''.join([char for char in texte if not char.isdigit()])\n",
        "    return texte\n",
        "\n",
        "# 2. Suppression des mots vides\n",
        "stop_words = set(stopwords.words('french'))\n",
        "def supprimer_stopwords(texte):\n",
        "    tokens = word_tokenize(texte)\n",
        "    return [mot for mot in tokens if mot not in stop_words]\n",
        "\n",
        "# 3. Lemmatisation\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatiser_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Pipeline complet\n",
        "texte_nettoye = nettoyer_texte(texte)\n",
        "tokens_sans_stopwords = supprimer_stopwords(texte_nettoye)\n",
        "tokens_lemmatise = lemmatiser_tokens(tokens_sans_stopwords)\n",
        "\n",
        "print(\"Texte nettoyé :\", texte_nettoye)\n",
        "print(\"Tokens après suppression des mots vides :\", tokens_sans_stopwords)\n",
        "print(\"Tokens après lemmatisation :\", tokens_lemmatise)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                          MultinomialNB                    GaussianNB  \\\n",
            "alpha              [0.1, 0.5, 1.0, 2.0]                           NaN   \n",
            "fit_prior                 [True, False]                           NaN   \n",
            "var_smoothing                       NaN  [1e-09, 1e-08, 1e-07, 1e-06]   \n",
            "binarize                            NaN                           NaN   \n",
            "penalty                             NaN                           NaN   \n",
            "C                                   NaN                           NaN   \n",
            "solver                              NaN                           NaN   \n",
            "max_iter                            NaN                           NaN   \n",
            "l1_ratio                            NaN                           NaN   \n",
            "n_neighbors                         NaN                           NaN   \n",
            "weights                             NaN                           NaN   \n",
            "algorithm                           NaN                           NaN   \n",
            "leaf_size                           NaN                           NaN   \n",
            "p                                   NaN                           NaN   \n",
            "criterion                           NaN                           NaN   \n",
            "max_depth                           NaN                           NaN   \n",
            "min_samples_split                   NaN                           NaN   \n",
            "min_samples_leaf                    NaN                           NaN   \n",
            "max_features                        NaN                           NaN   \n",
            "ccp_alpha                           NaN                           NaN   \n",
            "n_estimators                        NaN                           NaN   \n",
            "bootstrap                           NaN                           NaN   \n",
            "\n",
            "                            BernoulliNB                   LogisticRegression  \\\n",
            "alpha              [0.1, 0.5, 1.0, 2.0]                                  NaN   \n",
            "fit_prior                 [True, False]                                  NaN   \n",
            "var_smoothing                       NaN                                  NaN   \n",
            "binarize           [0.0, 0.5, 1.0, 2.0]                                  NaN   \n",
            "penalty                             NaN           [l1, l2, elasticnet, none]   \n",
            "C                                   NaN               [0.01, 0.1, 1.0, 10.0]   \n",
            "solver                              NaN  [liblinear, saga, lbfgs, newton-cg]   \n",
            "max_iter                            NaN                     [100, 500, 1000]   \n",
            "l1_ratio                            NaN                      [0.1, 0.5, 0.9]   \n",
            "n_neighbors                         NaN                                  NaN   \n",
            "weights                             NaN                                  NaN   \n",
            "algorithm                           NaN                                  NaN   \n",
            "leaf_size                           NaN                                  NaN   \n",
            "p                                   NaN                                  NaN   \n",
            "criterion                           NaN                                  NaN   \n",
            "max_depth                           NaN                                  NaN   \n",
            "min_samples_split                   NaN                                  NaN   \n",
            "min_samples_leaf                    NaN                                  NaN   \n",
            "max_features                        NaN                                  NaN   \n",
            "ccp_alpha                           NaN                                  NaN   \n",
            "n_estimators                        NaN                                  NaN   \n",
            "bootstrap                           NaN                                  NaN   \n",
            "\n",
            "                                KNeighborsClassifier DecisionTreeClassifier  \\\n",
            "alpha                                            NaN                    NaN   \n",
            "fit_prior                                        NaN                    NaN   \n",
            "var_smoothing                                    NaN                    NaN   \n",
            "binarize                                         NaN                    NaN   \n",
            "penalty                                          NaN                    NaN   \n",
            "C                                                NaN                    NaN   \n",
            "solver                                           NaN                    NaN   \n",
            "max_iter                                         NaN                    NaN   \n",
            "l1_ratio                                         NaN                    NaN   \n",
            "n_neighbors                            [3, 5, 7, 10]                    NaN   \n",
            "weights                          [uniform, distance]                    NaN   \n",
            "algorithm          [auto, ball_tree, kd_tree, brute]                    NaN   \n",
            "leaf_size                               [20, 30, 50]                    NaN   \n",
            "p                                             [1, 2]                    NaN   \n",
            "criterion                                        NaN        [gini, entropy]   \n",
            "max_depth                                        NaN     [None, 10, 20, 30]   \n",
            "min_samples_split                                NaN             [2, 5, 10]   \n",
            "min_samples_leaf                                 NaN              [1, 2, 5]   \n",
            "max_features                                     NaN     [None, sqrt, log2]   \n",
            "ccp_alpha                                        NaN      [0.0, 0.01, 0.05]   \n",
            "n_estimators                                     NaN                    NaN   \n",
            "bootstrap                                        NaN                    NaN   \n",
            "\n",
            "                  RandomForestClassifier  \n",
            "alpha                                NaN  \n",
            "fit_prior                            NaN  \n",
            "var_smoothing                        NaN  \n",
            "binarize                             NaN  \n",
            "penalty                              NaN  \n",
            "C                                    NaN  \n",
            "solver                               NaN  \n",
            "max_iter                             NaN  \n",
            "l1_ratio                             NaN  \n",
            "n_neighbors                          NaN  \n",
            "weights                              NaN  \n",
            "algorithm                            NaN  \n",
            "leaf_size                            NaN  \n",
            "p                                    NaN  \n",
            "criterion                [gini, entropy]  \n",
            "max_depth             [None, 10, 20, 30]  \n",
            "min_samples_split             [2, 5, 10]  \n",
            "min_samples_leaf               [1, 2, 5]  \n",
            "max_features          [sqrt, log2, None]  \n",
            "ccp_alpha              [0.0, 0.01, 0.05]  \n",
            "n_estimators             [100, 200, 500]  \n",
            "bootstrap                  [True, False]  \n"
          ]
        }
      ],
      "source": [
        "grid = pd.read_json(\"hyperpamètres.jsonl\")\n",
        "print(grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd2b2L2IAW4L"
      },
      "source": [
        "Exemple d'utilisation en temps normal pour l'optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FL1rQpFAdBF",
        "outputId": "e1ef638f-ab60-4fb7-c2d4-ad488d1ebfa0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.739\n"
          ]
        }
      ],
      "source": [
        "data = Data()\n",
        "data.lemmatisation(\"texte_annonce\")\n",
        "data.vectorisation_ponderee(\"texte_annonce\")\n",
        "data.reduction_nmf()\n",
        "data = data.get_data()\n",
        "\n",
        "foret = ForetAleatoire(data)\n",
        "foret.entrainement()\n",
        "print(foret.taux_reussite())\n",
        "print(foret.f1_score())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXVYhTmDL4Uu"
      },
      "source": [
        "Optimisation de la foret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYIVbtY9L4DU",
        "outputId": "2ead1a2b-580d-4159-c7db-c58e32aad6e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "etape 1\n",
            "etape 2\n",
            "etape 3\n",
            "etape 4\n",
            "etape 5\n",
            "etape 6\n",
            "Meilleurs hyperparamètres : {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 200}\n"
          ]
        }
      ],
      "source": [
        "#Donne les meilleurs hyperparametre de la foret\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "data = Data(limit = 100)\n",
        "print(\"etape 1\")\n",
        "data.lemmatisation(\"texte_annonce\")\n",
        "print(\"etape 2\")\n",
        "data.vectorisation_ponderee(\"texte_annonce\")\n",
        "print(\"etape 3\")\n",
        "data.reduction_nmf()\n",
        "print(\"etape 4\")\n",
        "data = data.get_data()\n",
        "print(\"etape 5\")\n",
        "\n",
        "foret = ForetAleatoire(data)\n",
        "print(\"etape 6\")\n",
        "foret.recherche_hyperparametres(param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Zy3czPCMQR7w"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\d.truong\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1770: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Taux de réussite : 0.735\n",
            "F1 score : 0.630\n"
          ]
        }
      ],
      "source": [
        "#Prédiction avec les meilleurs hyperparametre\n",
        "data = Data()\n",
        "data.lemmatisation(\"texte_annonce\")\n",
        "data.vectorisation_ponderee(\"texte_annonce\")\n",
        "data.reduction_nmf()\n",
        "data = data.get_data()\n",
        "\n",
        "foret = ForetAleatoire(data, max_depth = 20, max_features= None, min_samples_leaf= 5, min_samples_split= 5, n_estimators= 200)\n",
        "foret.entrainement()\n",
        "print(\"Taux de réussite :\", foret.taux_reussite())\n",
        "print(\"F1 score :\", foret.f1_score())\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
