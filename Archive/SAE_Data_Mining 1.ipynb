{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import des bibliothèque necessaires"
      ],
      "metadata": {
        "id": "wAeNBJIH4eo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD, NMF\n",
        "#nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "YXJFT3hINTdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48796fc-8c21-482c-c124-0fe21c1a86d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class Data pour tous les pré-traitement, vectorisation et réduction de la matrice."
      ],
      "metadata": {
        "id": "Oc4hk45n4kjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Data:\n",
        "    def __init__(self, limit=5000):\n",
        "        self.data_brut = pd.read_json(\"train.jsonl\", lines=True)[[\"texte_annonce\", \"cal_réponse_signalement\"]]\n",
        "        self.data = self.data_brut.iloc[:limit]\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.data\n",
        "\n",
        "    def rien(self, x):\n",
        "        pass\n",
        "\n",
        "    def racinisation(self, text_column):\n",
        "        stemmer = SnowballStemmer(\"french\")\n",
        "        self.data.loc[:, text_column] = self.data[text_column].apply(lambda x: ' '.join([stemmer.stem(word) for word in str(x).split()]))\n",
        "\n",
        "    def lemmatisation(self, text_column):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        self.data.loc[:, text_column] = self.data[text_column].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in str(x).split()]))\n",
        "\n",
        "    def vectorisation_simple(self, text_column):\n",
        "        vectorizer = CountVectorizer()\n",
        "        vect_data = vectorizer.fit_transform(self.data[text_column])\n",
        "        vect_df = pd.DataFrame(vect_data.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "        self.data = pd.concat([vect_df, self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def vectorisation_ponderee(self, text_column):\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        vect_data = vectorizer.fit_transform(self.data[text_column])\n",
        "        vect_df = pd.DataFrame(vect_data.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "        self.data = pd.concat([vect_df, self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def reduction_svd(self, n_components=100):\n",
        "        svd = TruncatedSVD(n_components=n_components)\n",
        "        svd_result = svd.fit_transform(self.data.iloc[:, :-1])\n",
        "        self.data = pd.concat([pd.DataFrame(svd_result), self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def reduction_nmf(self, n_components=100):\n",
        "        nmf = NMF(n_components=n_components)\n",
        "        nmf_result = nmf.fit_transform(self.data.iloc[:, :-1])\n",
        "        self.data = pd.concat([pd.DataFrame(nmf_result), self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def process_and_export(self, text_column=\"texte_annonce\", n_components=100):\n",
        "        # Définir toutes les combinaisons possibles\n",
        "        preprocessing_methods = [self.rien, self.racinisation, self.lemmatisation]\n",
        "        vectorisation_methods = [self.vectorisation_simple, self.vectorisation_ponderee]\n",
        "        reduction_methods = [self.reduction_svd, self.reduction_nmf]\n",
        "\n",
        "        combinations = product(preprocessing_methods, vectorisation_methods, reduction_methods)\n",
        "\n",
        "        # Boucler sur toutes les combinaisons\n",
        "        for i, (preprocess, vectorize, reduce) in enumerate(combinations):\n",
        "            # Réinitialiser les données à l'état brut\n",
        "            self.data = self.data_brut.copy()\n",
        "\n",
        "            # Appliquer les méthodes\n",
        "            preprocess(text_column)\n",
        "            vectorize(text_column)\n",
        "            reduce(n_components)\n",
        "\n",
        "            # Exporter le résultat\n",
        "            output_file = f\"output_combination_{i+1}.jsonl\"\n",
        "            self.data.to_json(output_file, orient=\"records\", lines=True)\n",
        "            print(f\"Exporté : {output_file}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-JQ20_eh4J4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class global des classifieurs permettant de faire de l'héritage"
      ],
      "metadata": {
        "id": "HFjpIvyd_Y5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifieur:\n",
        "    def __init__(self, data):\n",
        "        # Chargement et découpage des données\n",
        "        self.chargement(data)\n",
        "        self.decoupage()\n",
        "\n",
        "    def chargement(self, data):\n",
        "        # Chargement des données et séparation X, y\n",
        "        self.data = data\n",
        "        self.X = data.iloc[:, :-1]  # Toutes les colonnes sauf la dernière (X)\n",
        "        self.y = data.iloc[:, -1]   # Dernière colonne (y)\n",
        "\n",
        "    def decoupage(self):\n",
        "        # Découpage en jeu d'entrainement et test\n",
        "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.X, self.y, test_size=0.6, random_state=0)\n",
        "\n",
        "    def entrainement(self):\n",
        "        pass\n",
        "\n",
        "    def rien(self):\n",
        "        pass\n",
        "\n",
        "    def taux_reussite(self):\n",
        "        # Calcul du taux de réussite\n",
        "        y_pred = self.classifier.predict(self.X_test)\n",
        "        accuracy = accuracy_score(self.Y_test, y_pred)\n",
        "        return f\"{accuracy:.3f}\"\n",
        "\n",
        "    def f1_score(self):\n",
        "        # Calcul du F1-score\n",
        "        from sklearn.metrics import f1_score\n",
        "        y_pred = self.classifier.predict(self.X_test)\n",
        "        f1 = f1_score(self.Y_test, y_pred, average='weighted')\n",
        "        return f\"{f1:.3f}\"\n",
        "\n",
        "    def liste_reussite(self):\n",
        "        pass\n",
        "\n",
        "    def graph_train_vs_test(self, titre_graph, axe_x, nom_x, list_reussite_train, list_reussite_test):\n",
        "        # Affichage d'un graphique des taux de réussite pour l'entrainement et la validation\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(axe_x, list_reussite_train, label=f\"% réussite entrainement\", marker='o')\n",
        "        plt.plot(axe_x, list_reussite_test, label=f\"% réussite validation\", marker='o')\n",
        "        plt.xlabel(nom_x)\n",
        "        plt.ylabel(\"Taux de réussite\")\n",
        "        plt.title(titre_graph)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def graph_f1_train_vs_test(self, titre_graph, axe_x, nom_x, list_f1_train, list_f1_test):\n",
        "        # Affichage d'un graphique des F1-scores pour l'entrainement et la validation\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(axe_x, list_f1_train, label=f\"F1-score entraînement\", marker='o')\n",
        "        plt.plot(axe_x, list_f1_test, label=f\"F1-score validation\", marker='o')\n",
        "        plt.xlabel(nom_x)\n",
        "        plt.ylabel(\"F1-score\")\n",
        "        plt.title(titre_graph)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "RA_Ne8kbpOGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rajouter bernouillli et multit-nomiale\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
        "\n",
        "\n",
        "class BayesienNaif(Classifieur):\n",
        "    def __init__(self, data, mode=None, alpha = None, fit_prior = None, var_smoothing = None, binarize = None):\n",
        "        super().__init__(data)\n",
        "        self.mode = mode\n",
        "\n",
        "        if self.mode == 'multinomial':\n",
        "            self.classifier = MultinomialNB()\n",
        "            if alpha : self.classifier.alpha = alpha\n",
        "            if fit_prior : self.classifier.fit_prior = fit_prior\n",
        "        elif self.mode == 'bernoulli':\n",
        "            self.classifier = BernoulliNB()\n",
        "            if alpha : self.classifier.alpha = alpha\n",
        "            if fit_prior : self.classifier.fit_prior = fit_prior\n",
        "            if binarize : self.classifier.binarize = binarize\n",
        "        else:  # Par défaut, on utilise le modèle gaussien\n",
        "            self.classifier = GaussianNB()\n",
        "            if var_smoothing : self.classifier.var_smoothing = var_smoothing\n",
        "\n",
        "    def entrainement(self):\n",
        "        # Entraînement du modèle avec les données\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ],
      "metadata": {
        "id": "nAx2MGzSOn_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rajouter les régularisation\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "class RegressionLogistique(Classifieur):\n",
        "    def __init__(self, data, penalty = None, C = None, solver = None, max_iter = None, l1_ratio = None, random_state = 0):\n",
        "        super().__init__(data)\n",
        "        self.classifier = LogisticRegression()\n",
        "        if penalty : self.classifier.penalty = penalty\n",
        "        if C : self.classifier.C = C\n",
        "        if solver : self.classifier.solver = solver\n",
        "        if max_iter : self.classifier.max_iter = max_iter\n",
        "        if l1_ratio : self.classifier.l1_ratio\n",
        "        if random_state : self.classifier.random_state = random_state\n",
        "\n",
        "    def entrainement(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ],
      "metadata": {
        "id": "8fqw-IaPRTQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "class KPlusProchesVoisins(Classifieur):\n",
        "    def __init__(self, data, n_neighbors = None, weights = None, algorithm = None, p = None):\n",
        "        super().__init__(data)\n",
        "\n",
        "        self.classifier = KNeighborsClassifier()\n",
        "        if n_neighbors : self.classifier.n_neighbors = n_neighbors\n",
        "        if weights : self.classifier.weights = weights\n",
        "        if algorithm : self.classifier.algorithm = algorithm\n",
        "        if p : self.classifier.p = p\n",
        "\n",
        "    def entrainement(self, n_neighbors):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ],
      "metadata": {
        "id": "pE4gII77RjTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "class ArbreDeDecision(Classifieur):\n",
        "    def __init__(self, data, criterion = None, max_depth = None, min_samples_split = None, min_samples_leaf = None, max_features = None, ccp_alpha = None, random_state = 0):\n",
        "        super().__init__(data)\n",
        "        self.classifier = DecisionTreeClassifier()\n",
        "        if criterion : self.classifier.criterion = criterion\n",
        "        if max_depth : self.classifier.max_depth = max_depth\n",
        "        if min_samples_split : self.classifier.min_samples_split = min_samples_split\n",
        "        if min_samples_leaf : self.classifier.min_samples_leaf = min_samples_leaf\n",
        "        if max_features : self.classifier.max_features = max_features\n",
        "        if ccp_alpha : self.classifier.ccp_alpha = ccp_alpha\n",
        "        if random_state : self.classifier.random_state = random_state\n",
        "\n",
        "    def entrainement(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ],
      "metadata": {
        "id": "RPUV2vAXRx4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "class ForetAleatoire(Classifieur):\n",
        "    def __init__(self, data, n_estimators = None, criterion = None, max_depth = None, min_samples_split = None, min_samples_leaf = None, max_features = None, bootstrap = None, ccp_alpha = None, random_state = 0 ):\n",
        "        super().__init__(data)\n",
        "        self.classifier = RandomForestClassifier()\n",
        "        if n_estimators : self.classifier.n_estimators = n_estimators\n",
        "        if criterion : self.classifier.criterion = criterion\n",
        "        if max_depth : self.classifier.max_depth = max_depth\n",
        "        if min_samples_split : self.classifier.min_samples_split = min_samples_split\n",
        "        if min_samples_leaf : self.classifier.min_samples_leaf = min_samples_leaf\n",
        "        if max_features : self.classifier.max_features = max_features\n",
        "        if bootstrap : self.classifier.bootstrap = bootstrap\n",
        "        if ccp_alpha : self.classifier.ccp_alpha = ccp_alpha\n",
        "        if random_state : self.classifier.random_state = random_state\n",
        "\n",
        "    def entrainement(self, n_estimators=100):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ],
      "metadata": {
        "id": "t-mIMWs9R_AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonction ultra débile qui teste tous les classifieurs, pré traitements, vectorisation et réduction afin d'avoir le meilleurs taux de réussite."
      ],
      "metadata": {
        "id": "UeF-Q305_nzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def possibilite():\n",
        "    results = []  # Liste pour stocker les résultats\n",
        "\n",
        "    for x in [\"lemmatisation\", \"racinisation\",\"rien\"]:\n",
        "        for y in [\"vectorisation_simple\", \"vectorisation_ponderee\"]:\n",
        "            data = Data()\n",
        "            getattr(data, x)(\"texte_annonce\")\n",
        "            getattr(data, y)(\"texte_annonce\")\n",
        "\n",
        "            if y == \"vectorisation_ponderee\":\n",
        "                data.reduction_nmf()\n",
        "                z = \"nmf\"\n",
        "            else:\n",
        "                data.reduction_svd()\n",
        "                z = \"svd\"\n",
        "\n",
        "            # Parcours des classifieurs\n",
        "            for i in [BayesienNaif, RegressionLogistique, KPlusProchesVoisins, ArbreDeDecision, ForetAleatoire]:\n",
        "                classifieur = i(data.get_data())\n",
        "                classifieur.entrainement()\n",
        "                taux_reussite = classifieur.taux_reussite()\n",
        "                f1_score = classifieur.f1_score()\n",
        "\n",
        "                # Ajout des résultats à la liste sous forme de tuple\n",
        "                results.append((x, y, z, i.__name__, taux_reussite,f1_score))\n",
        "\n",
        "    # Tri des résultats par taux de réussite (du plus élevé au plus faible)\n",
        "    results_sorted = sorted(results, key=lambda x: x[4], reverse=True)\n",
        "\n",
        "    # Affichage des résultats triés\n",
        "    for res in results_sorted:\n",
        "        print(f\"{res[0]} - {res[1]} - {res[2]} - {res[3]} : {res[4]}\")\n",
        "\n",
        "possibilite()"
      ],
      "metadata": {
        "id": "HHJ6FYyH4au6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Résultat lancé en amont (tps: 16 min)\n",
        "\n",
        "- lemmatisation - vectorisation_ponderee - nmf - ForetAleatoire : 0.741\n",
        "- racinisation - vectorisation_ponderee - nmf - ForetAleatoire : 0.738\n",
        "- rien - vectorisation_ponderee - nmf - ForetAleatoire : 0.737\n",
        "- lemmatisation - vectorisation_ponderee - nmf - RegressionLogistique : 0.734\n",
        "- racinisation - vectorisation_simple - svd - ForetAleatoire : 0.734\n",
        "- racinisation - vectorisation_ponderee - nmf - RegressionLogistique : 0.734\n",
        "- rien - vectorisation_ponderee - nmf - RegressionLogistique : 0.734\n",
        "- lemmatisation - vectorisation_simple - svd - ForetAleatoire : 0.733\n",
        "- rien - vectorisation_simple - svd - ForetAleatoire : 0.732\n",
        "- lemmatisation - vectorisation_simple - svd - RegressionLogistique : 0.725\n",
        "- racinisation - vectorisation_simple - svd - RegressionLogistique : 0.723\n",
        "- rien - vectorisation_simple - svd - RegressionLogistique : 0.723\n",
        "- rien - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.667\n",
        "- lemmatisation - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.658\n",
        "- racinisation - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.657\n",
        "- racinisation - vectorisation_simple - svd - KPlusProchesVoisins : 0.651\n",
        "- lemmatisation - vectorisation_simple - svd - KPlusProchesVoisins : 0.648\n",
        "- rien - vectorisation_simple - svd - KPlusProchesVoisins : 0.644\n",
        "- lemmatisation - vectorisation_ponderee - nmf - ArbreDeDecision : 0.643\n",
        "- racinisation - vectorisation_ponderee - nmf - ArbreDeDecision : 0.640\n",
        "- rien - vectorisation_ponderee - nmf - ArbreDeDecision : 0.637\n",
        "- lemmatisation - vectorisation_simple - svd - ArbreDeDecision : 0.623\n",
        "- rien - vectorisation_simple - svd - ArbreDeDecision : 0.617\n",
        "- racinisation - vectorisation_simple - svd - ArbreDeDecision : 0.586\n",
        "- rien - vectorisation_ponderee - nmf - BayesienNaif : 0.494\n",
        "- lemmatisation - vectorisation_ponderee - nmf - BayesienNaif : 0.491\n",
        "- racinisation - vectorisation_ponderee - nmf - BayesienNaif : 0.483\n",
        "- lemmatisation - vectorisation_simple - svd - BayesienNaif : 0.329\n",
        "- rien - vectorisation_simple - svd - BayesienNaif : 0.328\n",
        "- racinisation - vectorisation_simple - svd - BayesienNaif : 0.326\n"
      ],
      "metadata": {
        "id": "NKATkwh7AhPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemple d'utilisation en temps normal pour l'optimisation"
      ],
      "metadata": {
        "id": "Bd2b2L2IAW4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = Data()\n",
        "data.lemmatisation(\"texte_annonce\")\n",
        "data.vectorisation_ponderee(\"texte_annonce\")\n",
        "data.reduction_nmf()\n",
        "data = data.get_data()\n",
        "\n",
        "foret = ForetAleatoire(data)\n",
        "foret.entrainement()\n",
        "print(foret.taux_reussite())\n",
        "print(foret.f1_score())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FL1rQpFAdBF",
        "outputId": "e1ef638f-ab60-4fb7-c2d4-ad488d1ebfa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimisation de la foret"
      ],
      "metadata": {
        "id": "KXVYhTmDL4Uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Donne les meilleurs hyperparametre de la foret\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "data = Data()\n",
        "data.lemmatisation(\"texte_annonce\")\n",
        "data.vectorisation_ponderee(\"texte_annonce\")\n",
        "data.reduction_nmf()\n",
        "data = data.get_data()\n",
        "\n",
        "foret = ForetAleatoire(data)\n",
        "foret.recherche_hyperparametres(param_grid)"
      ],
      "metadata": {
        "id": "lYIVbtY9L4DU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b76b6500-474e-401b-eb88-aaab47257180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-850db0b7c952>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatisation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"texte_annonce\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorisation_ponderee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"texte_annonce\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prédiction avec les meilleurs hyperparametre\n",
        "data = Data()\n",
        "data.lemmatisation(\"texte_annonce\")\n",
        "data.vectorisation_ponderee(\"texte_annonce\")\n",
        "data.reduction_nmf()\n",
        "data = data.get_data()\n",
        "\n",
        "foret = ForetAleatoire(data)\n",
        "foret.entrainement(max_depth=20, max_features=\"sqrt\", min_samples_leaf=2, min_samples_split=5, n_estimators=200)\n",
        "print(\"Taux de réussite :\", foret.taux_reussite())\n",
        "print(\"F1 score :\", foret.f1_score())\n",
        "\n"
      ],
      "metadata": {
        "id": "Zy3czPCMQR7w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}