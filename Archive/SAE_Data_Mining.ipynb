{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YXJFT3hINTdG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, limit=5000):\n",
    "        self.data_brut = pd.read_json(\"train.jsonl\", lines=True)[[\"texte_annonce\", \"cal_réponse_signalement\"]]\n",
    "        self.data = self.data_brut.iloc[:limit]\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "    def rien(self, x):\n",
    "        pass\n",
    "\n",
    "    def racinisation(self, text_column):\n",
    "        stemmer = SnowballStemmer(\"french\")\n",
    "        self.data.loc[:, text_column] = self.data[text_column].apply(lambda x: ' '.join([stemmer.stem(word) for word in str(x).split()]))\n",
    "\n",
    "    def lemmatisation(self, text_column):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        self.data.loc[:, text_column] = self.data[text_column].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in str(x).split()]))\n",
    "\n",
    "    def vectorisation_simple(self, text_column):\n",
    "        vectorizer = CountVectorizer()\n",
    "        vect_data = vectorizer.fit_transform(self.data[text_column])\n",
    "        vect_df = pd.DataFrame(vect_data.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "        self.data = pd.concat([vect_df, self.data.iloc[:, -1]], axis=1)\n",
    "\n",
    "    def vectorisation_ponderee(self, text_column):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vect_data = vectorizer.fit_transform(self.data[text_column])\n",
    "        vect_df = pd.DataFrame(vect_data.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "        self.data = pd.concat([vect_df, self.data.iloc[:, -1]], axis=1)\n",
    "\n",
    "    def reduction_svd(self, n_components=100):\n",
    "        svd = TruncatedSVD(n_components=n_components)\n",
    "        svd_result = svd.fit_transform(self.data.iloc[:, :-1])\n",
    "        self.data = pd.concat([pd.DataFrame(svd_result), self.data.iloc[:, -1]], axis=1)\n",
    "\n",
    "    def reduction_nmf(self, n_components=100):\n",
    "        nmf = NMF(n_components=n_components)\n",
    "        nmf_result = nmf.fit_transform(self.data.iloc[:, :-1])\n",
    "        self.data = pd.concat([pd.DataFrame(nmf_result), self.data.iloc[:, -1]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RA_Ne8kbpOGD"
   },
   "outputs": [],
   "source": [
    "class Classifieur:\n",
    "    def __init__(self, data):\n",
    "        # Chargement et découpage des données\n",
    "        self.chargement(data)\n",
    "        self.decoupage()\n",
    "\n",
    "    def chargement(self, data):\n",
    "        # Chargement des données et séparation X, y\n",
    "        self.data = data\n",
    "        self.X = data.iloc[:, :-1]  # Toutes les colonnes sauf la dernière (X)\n",
    "        self.y = data.iloc[:, -1]   # Dernière colonne (y)\n",
    "\n",
    "    def decoupage(self):\n",
    "        # Découpage en jeu d'entrainement et test\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.X, self.y, test_size=0.6, random_state=0)\n",
    "\n",
    "    def entrainement(self):\n",
    "        pass\n",
    "\n",
    "    def rien(self):\n",
    "        pass\n",
    "    \n",
    "    def taux_reussite(self):\n",
    "        # Calcul du taux de réussite\n",
    "        y_pred = self.classifier.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.Y_test, y_pred)\n",
    "        return f\"{accuracy:.3f}\"\n",
    "\n",
    "    def liste_reussite(self):\n",
    "        pass\n",
    "\n",
    "    def graph_train_vs_test(self, titre_graph, axe_x, nom_x, list_reussite_train, list_reussite_test):\n",
    "        # Affichage d'un graphique des taux de réussite pour l'entrainement et la validation\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(axe_x, list_reussite_train, label=f\"% réussite entrainement\", marker='o')\n",
    "        plt.plot(axe_x, list_reussite_test, label=f\"% réussite validation\", marker='o')\n",
    "        plt.xlabel(nom_x)\n",
    "        plt.ylabel(\"Taux de réussite\")\n",
    "        plt.title(titre_graph)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nAx2MGzSOn_S"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "class BayesienNaif(Classifieur):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "\n",
    "    def entrainement(self):\n",
    "        self.classifier = GaussianNB()\n",
    "        self.classifier.fit(self.X_train, self.Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8fqw-IaPRTQ0"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class RegressionLogistique(Classifieur):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "\n",
    "    def entrainement(self):\n",
    "        self.classifier = LogisticRegression(max_iter=1000, random_state=0)\n",
    "        self.classifier.fit(self.X_train, self.Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pE4gII77RjTM"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "class KPlusProchesVoisins(Classifieur):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "\n",
    "    def entrainement(self, n_neighbors=3):\n",
    "        self.classifier = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        self.classifier.fit(self.X_train, self.Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "RPUV2vAXRx4G"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class ArbreDeDecision(Classifieur):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "\n",
    "    def entrainement(self):\n",
    "        self.classifier = DecisionTreeClassifier(random_state=0)\n",
    "        self.classifier.fit(self.X_train, self.Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "t-mIMWs9R_AW"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class ForetAleatoire(Classifieur):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "\n",
    "    def entrainement(self, n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features='sqrt'):\n",
    "        self.classifier = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=0\n",
    "        )\n",
    "        self.classifier.fit(self.X_train, self.Y_train)\n",
    "    \n",
    "    def recherche_hyperparametres(self, param_grid):\n",
    "        # Recherche des meilleurs hyperparamètres avec GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            RandomForestClassifier(random_state=0),\n",
    "            param_grid,\n",
    "            cv=5,  # Validation croisée 5-fold\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-6  # Utilisation maximale des cœurs CPU\n",
    "        )\n",
    "        grid_search.fit(self.X_train, self.Y_train)\n",
    "        self.classifier = grid_search.best_estimator_\n",
    "        print(\"Meilleurs hyperparamètres :\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d.truong\\AppData\\Local\\Temp\\ipykernel_12624\\1930870088.py:3: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  self.data_brut = pd.read_json(\"train (1).jsonl\", lines=True)[[\"texte_annonce\", \"cal_réponse_signalement\"]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected character found when decoding 'true'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results_sorted:\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m possibilite()\n",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m, in \u001b[0;36mpossibilite\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatisation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mracinisation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrien\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectorisation_simple\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectorisation_ponderee\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m----> 6\u001b[0m         data \u001b[38;5;241m=\u001b[39m Data()\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(data, x)(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtexte_annonce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(data, y)(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtexte_annonce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m, in \u001b[0;36mData.__init__\u001b[1;34m(self, limit)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_brut \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain (1).jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtexte_annonce\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcal_réponse_signalement\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_brut\u001b[38;5;241m.\u001b[39miloc[:limit]\n",
      "File \u001b[1;32mc:\\Users\\d.truong\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\d.truong\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1023\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1021\u001b[0m         data \u001b[38;5;241m=\u001b[39m ensure_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m   1022\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\d.truong\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m FrameParser(json, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mparse()\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\d.truong\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse()\n\u001b[0;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\d.truong\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1403\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1399\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[1;32m-> 1403\u001b[0m         ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m     )\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1406\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1407\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[0;32m   1408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1409\u001b[0m     }\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected character found when decoding 'true'"
     ]
    }
   ],
   "source": [
    "def possibilite():\n",
    "    results = []  # Liste pour stocker les résultats\n",
    "    \n",
    "    for x in [\"lemmatisation\", \"racinisation\",\"rien\"]:\n",
    "        for y in [\"vectorisation_simple\", \"vectorisation_ponderee\"]:\n",
    "            data = Data()\n",
    "            getattr(data, x)(\"texte_annonce\")\n",
    "            getattr(data, y)(\"texte_annonce\")\n",
    "\n",
    "            if y == \"vectorisation_ponderee\":\n",
    "                data.reduction_nmf()\n",
    "                z = \"nmf\"\n",
    "            else:\n",
    "                data.reduction_svd()\n",
    "                z = \"svd\"\n",
    "\n",
    "            # Parcours des classifieurs\n",
    "            for i in [BayesienNaif, RegressionLogistique, KPlusProchesVoisins, ArbreDeDecision, ForetAleatoire]:\n",
    "                classifieur = i(data.get_data())\n",
    "                classifieur.entrainement()\n",
    "                taux_reussite = classifieur.taux_reussite()\n",
    "                \n",
    "                # Ajout des résultats à la liste sous forme de tuple\n",
    "                results.append((x, y, z, i.__name__, taux_reussite))\n",
    "\n",
    "    # Tri des résultats par taux de réussite (du plus élevé au plus faible)\n",
    "    results_sorted = sorted(results, key=lambda x: x[4], reverse=True)\n",
    "    \n",
    "    # Affichage des résultats triés\n",
    "    for res in results_sorted:\n",
    "        print(f\"{res[0]} - {res[1]} - {res[2]} - {res[3]} : {res[4]}\")\n",
    "\n",
    "possibilite()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatisation - vectorisation_ponderee - nmf - ForetAleatoire : 0.741\n",
    "racinisation - vectorisation_ponderee - nmf - ForetAleatoire : 0.738\n",
    "rien - vectorisation_ponderee - nmf - ForetAleatoire : 0.737\n",
    "lemmatisation - vectorisation_ponderee - nmf - RegressionLogistique : 0.734\n",
    "racinisation - vectorisation_simple - svd - ForetAleatoire : 0.734\n",
    "racinisation - vectorisation_ponderee - nmf - RegressionLogistique : 0.734\n",
    "rien - vectorisation_ponderee - nmf - RegressionLogistique : 0.734\n",
    "lemmatisation - vectorisation_simple - svd - ForetAleatoire : 0.733\n",
    "rien - vectorisation_simple - svd - ForetAleatoire : 0.732\n",
    "lemmatisation - vectorisation_simple - svd - RegressionLogistique : 0.725\n",
    "racinisation - vectorisation_simple - svd - RegressionLogistique : 0.723\n",
    "rien - vectorisation_simple - svd - RegressionLogistique : 0.723\n",
    "rien - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.667\n",
    "lemmatisation - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.658\n",
    "racinisation - vectorisation_ponderee - nmf - KPlusProchesVoisins : 0.657\n",
    "racinisation - vectorisation_simple - svd - KPlusProchesVoisins : 0.651\n",
    "lemmatisation - vectorisation_simple - svd - KPlusProchesVoisins : 0.648\n",
    "rien - vectorisation_simple - svd - KPlusProchesVoisins : 0.644\n",
    "lemmatisation - vectorisation_ponderee - nmf - ArbreDeDecision : 0.643\n",
    "racinisation - vectorisation_ponderee - nmf - ArbreDeDecision : 0.640\n",
    "rien - vectorisation_ponderee - nmf - ArbreDeDecision : 0.637\n",
    "lemmatisation - vectorisation_simple - svd - ArbreDeDecision : 0.623\n",
    "rien - vectorisation_simple - svd - ArbreDeDecision : 0.617\n",
    "racinisation - vectorisation_simple - svd - ArbreDeDecision : 0.586\n",
    "rien - vectorisation_ponderee - nmf - BayesienNaif : 0.494\n",
    "lemmatisation - vectorisation_ponderee - nmf - BayesienNaif : 0.491\n",
    "racinisation - vectorisation_ponderee - nmf - BayesienNaif : 0.483\n",
    "lemmatisation - vectorisation_simple - svd - BayesienNaif : 0.329\n",
    "rien - vectorisation_simple - svd - BayesienNaif : 0.328\n",
    "racinisation - vectorisation_simple - svd - BayesienNaif : 0.326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "data = Data()\n",
    "data.lemmatisation(\"texte_annonce\")\n",
    "data.vectorisation_ponderee(\"texte_annonce\")\n",
    "data.reduction_nmf()\n",
    "data = data.get_data()\n",
    "\n",
    "foret = ForetAleatoire(data)\n",
    "foret.recherche_hyperparametres(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "data.lemmatisation(\"texte_annonce\")\n",
    "data.vectorisation_ponderee(\"texte_annonce\")\n",
    "data.reduction_nmf()\n",
    "data = data.get_data()\n",
    "\n",
    "foret = ForetAleatoire(data)\n",
    "foret.entrainement(max_depth=20, max_features=\"sqrt\", min_samples_leaf=2, min_samples_split=5, n_estimators=200)  \n",
    "print(\"Taux de réussite :\", foret.taux_reussite())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
