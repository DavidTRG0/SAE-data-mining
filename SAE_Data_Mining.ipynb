{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAeNBJIH4eo7"
      },
      "source": [
        "Import des bibliothèque necessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXJFT3hINTdG",
        "outputId": "1fbc9125-d7af-4241-ecd5-93fa5859be1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Brasi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Brasi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importations standard\n",
        "import json\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Importations de bibliothèques tierces\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product\n",
        "\n",
        "# Importations des modules sklearn\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD, NMF\n",
        "\n",
        "# Importations nltk\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Vérifier et télécharger les ressources nltk si nécessaires\n",
        "nltk_resources = ['stopwords', 'punkt', 'wordnet']\n",
        "for resource in nltk_resources:\n",
        "    try:\n",
        "        nltk.data.find(f'tokenizers/{resource}' if resource == 'punkt' else f'corpora/{resource}')\n",
        "    except LookupError:\n",
        "        nltk.download(resource)\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc4hk45n4kjt"
      },
      "source": [
        "Classe Data pour tous les pré-traitement, vectorisation et réduction de la matrice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-JQ20_eh4J4b"
      },
      "outputs": [],
      "source": [
        "class Data:\n",
        "    def __init__(self, data = \"train.jsonl\"  , limit=5000, language=\"french\", text_column=\"texte_annonce\"):\n",
        "        self.data = (pd.read_json(data, lines=True)[[text_column, \"cal_réponse_signalement\"]]).iloc[:limit]\n",
        "        self.data[\"cal_réponse_signalement\"] = self.data[\"cal_réponse_signalement\"].map({\"Pris en compte\": 0,\"Rejete (hors specs)\": 1})\n",
        "        self.text_column = text_column\n",
        "        self.language = language\n",
        "\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.data\n",
        "\n",
        "    def supprimer_stopwords(self):\n",
        "        stop_words = stopwords.words(self.language)\n",
        "\n",
        "        def nettoyer_texte(texte):\n",
        "            texte = re.sub(r'[^\\w\\s]', '', texte)  # Retirer la ponctuation\n",
        "            texte = re.sub(r'\\d+', '', texte)      # Retirer les chiffres\n",
        "            tokens = nltk.word_tokenize(texte.lower())\n",
        "            return ' '.join([word for word in tokens if word not in stop_words])\n",
        "\n",
        "        # Appliquer la fonction de nettoyage à la colonne texte\n",
        "        self.data[self.text_column] = self.data[self.text_column].apply(nettoyer_texte)\n",
        "\n",
        "    def rien(self, x):\n",
        "        pass\n",
        "\n",
        "    def racinisation(self, ignore_stopwords=None):\n",
        "        stemmer = SnowballStemmer(self.language)\n",
        "        self.data[self.text_column] = self.data[self.text_column].apply(lambda x: ' '.join([stemmer.stem(word) for word in str(x).split()]))\n",
        "        self.supprimer_stopwords()\n",
        "\n",
        "    def lemmatisation(self, mode=None, overwrite=None):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        if mode:\n",
        "            lemmatizer.mode = mode\n",
        "        if overwrite:\n",
        "            lemmatizer.overwrite = overwrite\n",
        "        self.data[self.text_column] = self.data[self.text_column].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in str(x).split()]))\n",
        "        self.supprimer_stopwords()\n",
        "\n",
        "    def vectorisation_simple(self, ngram_range=None, max_features=None):\n",
        "        vectorizer = CountVectorizer()\n",
        "        if ngram_range:\n",
        "            vectorizer.ngram_range = ngram_range\n",
        "        if max_features:\n",
        "            vectorizer.max_features = max_features\n",
        "        vect_data = vectorizer.fit_transform(self.data[self.text_column])\n",
        "        vect_df = pd.DataFrame(vect_data.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "        self.data = pd.concat([vect_df, self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def vectorisation_ponderee(self, ngram_range=None, max_features=None, norm=None):\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        if ngram_range:\n",
        "            vectorizer.ngram_range = ngram_range\n",
        "        if max_features:\n",
        "            vectorizer.max_features = max_features\n",
        "        if norm:\n",
        "            vectorizer.norm = norm\n",
        "        vect_data = vectorizer.fit_transform(self.data[self.text_column])\n",
        "        vect_df = pd.DataFrame(vect_data.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "        self.data = pd.concat([vect_df, self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def reduction_svd(self, n_components=None, n_iter=None):\n",
        "        svd = TruncatedSVD()\n",
        "        if n_components:\n",
        "            svd.n_components = n_components\n",
        "        if n_iter:\n",
        "            svd.n_iter = n_iter\n",
        "        svd = TruncatedSVD(n_components=n_components)\n",
        "        svd_result = svd.fit_transform(self.data.iloc[:, :-1])\n",
        "        self.data = pd.concat([pd.DataFrame(svd_result), self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def reduction_nmf(self, n_components=50, init=None):\n",
        "        nmf = NMF(n_components=n_components)\n",
        "        if init:\n",
        "            nmf.init = init\n",
        "        nmf_result = nmf.fit_transform(self.data.iloc[:, :-1])\n",
        "        self.data = pd.concat([pd.DataFrame(nmf_result), self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def process_and_export(self, n_components=100):\n",
        "        # Définir toutes les combinaisons possibles\n",
        "        preprocessing_methods = [self.rien, self.racinisation, self.lemmatisation]\n",
        "        vectorisation_methods = [self.vectorisation_simple, self.vectorisation_ponderee]\n",
        "        reduction_methods = [self.reduction_svd, self.reduction_nmf]\n",
        "\n",
        "        combinations = product(preprocessing_methods, vectorisation_methods, reduction_methods)\n",
        "\n",
        "        # Boucler sur toutes les combinaisons\n",
        "        for i, (preprocess, vectorize, reduce) in enumerate(combinations):\n",
        "            # Réinitialiser les données à l'état brut\n",
        "            self.data = self.data_brut.copy()\n",
        "\n",
        "            # Appliquer les méthodes\n",
        "            preprocess(self.text_column)\n",
        "            vectorize(self.text_column)\n",
        "            reduce(n_components)\n",
        "\n",
        "            # Exporter le résultat\n",
        "            output_file = f\"output_combination_{i+1}.jsonl\"\n",
        "            self.data.to_json(output_file, orient=\"records\", lines=True)\n",
        "            print(f\"Exporté : {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFjpIvyd_Y5S"
      },
      "source": [
        "Classe global des classifieurs permettant de faire de l'héritage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RA_Ne8kbpOGD"
      },
      "outputs": [],
      "source": [
        "class Classifieur:\n",
        "    def __init__(self, data):\n",
        "        # Chargement et découpage des données\n",
        "        self.chargement(data)\n",
        "        self.decoupage()\n",
        "\n",
        "    def chargement(self, data):\n",
        "        # Chargement des données et séparation X, y\n",
        "        self.data = data\n",
        "        self.X = data.iloc[:, :-1]  # Toutes les colonnes sauf la dernière (X)\n",
        "        self.y = data.iloc[:, -1]   # Dernière colonne (y)\n",
        "\n",
        "    def decoupage(self):\n",
        "        # Découpage en jeu d'entrainement et test\n",
        "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.X, self.y, test_size=0.6, random_state=0)\n",
        "\n",
        "    def entrainement(self):\n",
        "        pass\n",
        "\n",
        "    def rien(self):\n",
        "        pass\n",
        "\n",
        "    def taux_reussite(self):\n",
        "        # Calcul du taux de réussite\n",
        "        y_pred = self.classifier.predict(self.X_test)\n",
        "        accuracy = accuracy_score(self.Y_test, y_pred)\n",
        "        return f\"{accuracy:.3f}\"\n",
        "\n",
        "    def f1_score(self):\n",
        "        # Calcul du F1-score\n",
        "        from sklearn.metrics import f1_score\n",
        "        y_pred = self.classifier.predict(self.X_test)\n",
        "        f1 = f1_score(self.Y_test, y_pred, average='weighted')\n",
        "        return f\"{f1:.3f}\"\n",
        "\n",
        "    def recherche_hyperparametres(self,param_grid):\n",
        "          # Recherche des meilleurs hyperparamètres avec GridSearchCV\n",
        "          grid_search = GridSearchCV(\n",
        "              self.classifier,\n",
        "              param_grid,\n",
        "              cv=5,  # Validation croisée 5-fold\n",
        "              scoring='f1',\n",
        "          )\n",
        "          grid_search.fit(self.X_train, self.Y_train) # tres important de preciser le label posit\n",
        "          self.classifier = grid_search.best_estimator_ # Automatiquement garde le meilleur classifieur\n",
        "          print(\"Meilleurs hyperparamètres :\", grid_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nAx2MGzSOn_S"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "\n",
        "class Multinomial(Classifieur):\n",
        "    def __init__(self, data, alpha = None, fit_prior = None):\n",
        "        super().__init__(data)\n",
        "        self.classifier = MultinomialNB()\n",
        "        if alpha : self.classifier.alpha = alpha\n",
        "        if fit_prior : self.classifier.fit_prior = fit_prior\n",
        "\n",
        "    def entrainement(self):\n",
        "        # Entraînement du modèle avec les données\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Mbtkcyg7njLK"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "\n",
        "class Bernoulli(Classifieur):\n",
        "    def __init__(self, data, alpha = None, fit_prior = None, binarize = None):\n",
        "        super().__init__(data)\n",
        "\n",
        "        self.classifier = BernoulliNB()\n",
        "        if alpha : self.classifier.alpha = alpha\n",
        "        if fit_prior : self.classifier.fit_prior = fit_prior\n",
        "        if binarize : self.classifier.binarize = binarize\n",
        "\n",
        "\n",
        "    def entrainement(self):\n",
        "        # Entraînement du modèle avec les données\n",
        "        self.classifier.fit(self.X_train, self.Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5Kpxa-J6njnS"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "class Gaussian(Classifieur):\n",
        "    def __init__(self, data, var_smoothing = None):\n",
        "        super().__init__(data)\n",
        "        self.classifier = GaussianNB()\n",
        "        if var_smoothing : self.classifier.var_smoothing = var_smoothing\n",
        "\n",
        "    def entrainement(self):\n",
        "        # Entraînement du modèle avec les données\n",
        "        self.classifier.fit(self.X_train, self.Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8fqw-IaPRTQ0"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "class RegressionLogistique(Classifieur):\n",
        "    def __init__(self, data, penalty = None, C = None, solver = None, max_iter = None, l1_ratio = None, random_state = None):\n",
        "        super().__init__(data)\n",
        "        self.classifier = LogisticRegression()\n",
        "        if penalty : self.classifier.penalty = penalty\n",
        "        if C : self.classifier.C = C\n",
        "        if solver : self.classifier.solver = solver\n",
        "        if max_iter : self.classifier.max_iter = max_iter\n",
        "        if l1_ratio : self.classifier.l1_ratio\n",
        "        if random_state : self.classifier.random_state = random_state\n",
        "\n",
        "    def entrainement(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pE4gII77RjTM"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "class KPlusProchesVoisins(Classifieur):\n",
        "    def __init__(self, data, n_neighbors = 3, weights = None, algorithm = None, p = None):\n",
        "        super().__init__(data)\n",
        "\n",
        "        self.classifier = KNeighborsClassifier()\n",
        "        if n_neighbors : self.classifier.n_neighbors = n_neighbors\n",
        "        if weights : self.classifier.weights = weights\n",
        "        if algorithm : self.classifier.algorithm = algorithm\n",
        "        if p : self.classifier.p = p\n",
        "\n",
        "    def entrainement(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RPUV2vAXRx4G"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "class ArbreDeDecision(Classifieur):\n",
        "    def __init__(self, data, criterion = None, max_depth = None, min_samples_split = None, min_samples_leaf = None, max_features = None, ccp_alpha = None, random_state = None):\n",
        "        super().__init__(data)\n",
        "        self.classifier = DecisionTreeClassifier()\n",
        "        if criterion : self.classifier.criterion = criterion\n",
        "        if max_depth : self.classifier.max_depth = max_depth\n",
        "        if min_samples_split : self.classifier.min_samples_split = min_samples_split\n",
        "        if min_samples_leaf : self.classifier.min_samples_leaf = min_samples_leaf\n",
        "        if max_features : self.classifier.max_features = max_features\n",
        "        if ccp_alpha : self.classifier.ccp_alpha = ccp_alpha\n",
        "        if random_state : self.classifier.random_state = random_state\n",
        "\n",
        "    def entrainement(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "t-mIMWs9R_AW"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "class ForetAleatoire(Classifieur):\n",
        "    def __init__(self, data, n_estimators = 100, criterion = None, max_depth = None, min_samples_split = None, min_samples_leaf = None, max_features = None, bootstrap = None, ccp_alpha = None, random_state = 0 ):\n",
        "        super().__init__(data)\n",
        "        self.classifier = RandomForestClassifier()\n",
        "        if n_estimators : self.classifier.n_estimators = n_estimators\n",
        "        if criterion : self.classifier.criterion = criterion\n",
        "        if max_depth : self.classifier.max_depth = max_depth\n",
        "        if min_samples_split : self.classifier.min_samples_split = min_samples_split\n",
        "        if min_samples_leaf : self.classifier.min_samples_leaf = min_samples_leaf\n",
        "        if max_features : self.classifier.max_features = max_features\n",
        "        if bootstrap : self.classifier.bootstrap = bootstrap\n",
        "        if ccp_alpha : self.classifier.ccp_alpha = ccp_alpha\n",
        "        if random_state : self.classifier.random_state = random_state\n",
        "\n",
        "    def entrainement(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZZimGH9RLKG",
        "outputId": "148e89cb-75dd-4a28-b591-473eeeb73313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combo data généré,  nombre : 48\n",
            "{'preprocessing': {'method': 'lemmatisation', 'parameters': {'mode': 'lookup', 'overwrite': True}}, 'vectorization': {'method': 'vectorisation_ponderee', 'parameters': {'ngram_range': [1, 1], 'max_features': None, 'norm': 'l2'}}, 'dimension_reduction': None}\n",
            "{'{\"method\": \"lemmatisation\", \"parameters\": {\"mode\": \"lookup\", \"overwrite\": true}}': <__main__.Data object at 0x0000022CD56CAA50>}\n",
            "{'{\"method\": \"lemmatisation\", \"parameters\": {\"mode\": \"lookup\", \"overwrite\": true}},{\"method\": \"vectorisation_ponderee\", \"parameters\": {\"ngram_range\": [1, 1], \"max_features\": null, \"norm\": \"l2\"}}': <__main__.Data object at 0x0000022CD56CAA50>}\n",
            "{'{\"method\": \"lemmatisation\", \"parameters\": {\"mode\": \"lookup\", \"overwrite\": true}},{\"method\": \"vectorisation_ponderee\", \"parameters\": {\"ngram_range\": [1, 1], \"max_features\": null, \"norm\": \"l2\"}},null,{\"method\": \"vectorisation_ponderee\", \"parameters\": {\"ngram_range\": [1, 1], \"max_features\": null, \"norm\": \"l2\"}}': <__main__.Data object at 0x0000022CD56CAA50>}\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\nAll the 19440 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3888 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 363, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1012, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 751, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'avis marché attention information contenues lextrait pdf peuvent certains ca pa présenter texte intégral lannonce extraits pdf annonces boamp constituent pa format officiel consulter texte intégral format officiel présent avis cliquez httpswwwboampfrpagesavis qidweb départements publication annonce travaux section identification lacheteur conseil départemental charente nom complet lacheteur siret type numéro national dindentification national didentification angouleme ville code postal non groupement commandes départements publication section communication lien direct document consultation httpswwwmarchespublicsinfompiawsindexcfm fuseactiondematentlogintypedceidm identifiant interne consultation oui intégralité document profil dacheteur non utilisation moyens communication non communément disponibles stéphane quelard nom contact numéro téléphone contact section procédure procédure adaptée ouverte type procédure conditions participation formulaire dc aptitude exercer lactivité professionnelle condition moyens preuve lettre candidature _ habilitation mandataire cotraitants disponible ladresse suivante httpwwweconomiegouvfrdajformulairesdeclarationducandidat formulaire dc déclaration candidat individuel membre groupement disponible ladresse suivante httpwwweconomiegouvfrdajformulairesdeclarationducandidat copie jugements prononcés si candidat redressement judiciaire déclaration appropriée capacité économique financière condition moyens preuve banques preuve dune assurance risques professionnels présentation dune capacités technique professionnelles condition moyens preuve liste travaux exécutés cours cinq dernières années appuyée dattestations bonne exécution travaux plus importants déclaration indiquant loutillage matériel léquipement technique dont candidat dispose réalisation marchés nature déclaration indiquant effectifs moyens annuels candidat limportance personnel dencadrement chacune trois dernières années sans objet technique dachat date heure limite réception plis autorisée présentation offres catalogue électronique non réduction nombre candidats oui possibilité dattribution sans négociation non lacheteur exige présentations variantes valeur technique loffre délai dexécution prix critères dattribution section identification marché aménagement carrefour rd vc bouèges intitulé marché code cpv principal descripteur principal travaux type marché aménagement dun nouveau carrefour entre rd vc description succincte marché bouèges raccordement dun chemin rural fermeture carrefour rd comprenant décapage terre végétale terrassement déblais rabotage scarification chaussée fond forme remblais couche forme couche réglage mise oeuvre bordures a déblais mi remblais création regards pose canalisation têtes buse mise oeuvre gb bbsg création cunettes bétonnées u création fossé engazonnement carrefour rd vc bouèges saintsornin lieu principal dexécution marché durée marché mois non consultation comporte tranche non consultation prévoit réservation tout partie marché non marché alloti section informations complementaires non visite obligatoire présente consultation passée application article autres information complémentaires r r r r code commande publique relatifs dématérialisation procédures passation marchés publics candidats donc possibilité rendant site internet collectivité ladresse httpwwwlacharentefr rubrique marchés public httpmarchespublicslacharentefr consulter avis publicité règlement consultation télécharger dce intégralité poser question dce répondre voie électronique être tenu informés lévolution consultation rectificatif information complémentaires rejet candidature offres date denvoi présent avis publication'\n\n--------------------------------------------------------------------------------\n15552 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 363, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1012, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 751, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'avis attention information contenues lextrait pdf peuvent certains ca pa présenter texte intégral lannonce extraits pdf annonces boamp constituent pa format officiel consulter texte intégral format officiel présent avis cliquez httpswwwboampfrpagesavisqidweb départements publication annonce no i ii iii iv v vi fns simple avis marché section i identification lacheteur nom complet lacheteur cantal habitat type numéro national dindentification siret national didentification ville aurillac code postal groupement commandes non départements publication section communication lien direct document consultation httpswwwmarchespublicsinfompiawsindexcfm fuseactiondematentlogintypedceidm identifiant interne consultation dce_cs_haras intégralité document profil dacheteur oui utilisation moyens communication non communément disponibles non nom contact service marchés publics adresse mail contact numéro téléphone contact section procedure type procédure procédure adaptée ouverte conditions participation aptitude exercer lactivité professionnelle condition moyens preuve formulaire dc lettre candidature _ habilitation mandataire co traitantsdisponible ladresse suivante httpwwweconomiegouvfrdajformulairesdeclarationducandidat formulaire dc déclaration candidat individuel membre groupementdisponible ladresse suivante httpwwweconomiegouvfrdajformulairesdeclarationdu candidatdéclaration lhonneur justifier candidat nentre aucun ca dinterdiction soumissionner capacité économique financière condition moyens preuve déclaration concernant chiffre daffaires global chiffre daffaires concernant prestations objet contrat réalisées cours trois derniers exercices disponibles capacités technique professionnelles condition moyens preuve déclaration indiquant effectifs moyens annuels candidat limportance personnel dencadrement chacune trois dernières annéesdéclaration indiquant loutillage matériel léquipement technique dont candidat dispose réalisation contratliste travaux exécutés cours cinq dernières années appuyée dattestations bonne exécution plus importants montant époque lieu dexécution sils effectués selon règles lart menés bonne fin technique dachat sans objet date heure limite réception plis octobre présentation offres catalogue électronique interdite réduction nombre candidats non possibilité dattribution sans négociation oui lacheteur exige présentations variantes non identification catégories dacheteurs intervenant si accordcadre critères dattribution valeur technique loffre appréciée laide cadre réponse technique pts prix section identification marché intitulé marché travaux daménagement secteur haras aurillac code cpv principal descripteur principal type marché travaux description succincte marché travaux daménagement secteur haras aurillaclopération décomposée phase phase desserte vrd logements construction chemin piéton mois dexécutionphase aménagement extérieur lensemble hors village haras mois dexécutionphase réhabilitation aménagements extérieurs village haras mois dexécutionet dispose pse comme suit pse fourniture pose bordures phases pse réhabilitation aep village haras phase pse eclairage public phases pse aménagement dune aire jeux phase durée prévisionnelle mois dont mois période préparation chantierla consultation fait lobjet dune visite obligatoire dont condition exposées règlement consultationle marché peut faire lobjet négociation lieu principal dexécution marché village haras avenue julien aurillac durée marché mois valeur estimée ht valeur entre consultation comporte tranche non consultation prévoit réservation tout partie marché non marché alloti non mots descripteurs voirie voirie réseaux diver section lots bloc nest pa accessible car lobjet nest pa alloti section informations complementaires visite obligatoire oui détail visite si oui référer larticle rc autres information complémentaires signature électronique nest pa exigéeune garantie financière prévue contrat contrat prévoit versement dune avance obligation constituer garantie première demande contrepartieles prix révisablesle paiement prestations fera respect délai global paiement applicable lacheteurle marché financé fonds propres cantal habitatles voies recours ouvertes candidats suivantes référé précontractuel prévu article code justice administrative cja pouvant être exercé avant signature contrat référé contractuel prévu article cja pouvant être exercé délais prévus larticle r cja recours pleine juridiction ouvert concurrents évincés pouvant être exercé deux mois suivant date laquelle conclusion contrat rendue publique recours contreune décision administrative prévu article r r cja pouvant être exercé mois suivant notification publication décision lorganisme date denvoi présent avis septembre'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[38], line 199\u001b[0m\n\u001b[0;32m    194\u001b[0m         param_combos \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(keys, combination)) \u001b[38;5;28;01mfor\u001b[39;00m combination \u001b[38;5;129;01min\u001b[39;00m product(\u001b[38;5;241m*\u001b[39mvalues)]\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m param_combos\n\u001b[1;32m--> 199\u001b[0m projet \u001b[38;5;241m=\u001b[39m \u001b[43mProjet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[38], line 47\u001b[0m, in \u001b[0;36mProjet.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39metape_1_2_3_sauv \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_res \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[38], line 88\u001b[0m, in \u001b[0;36mProjet.main_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m classifieur_name \u001b[38;5;241m=\u001b[39m classifieur\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m     87\u001b[0m param_classifieur \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyper_classifieur[classifieur_name]\n\u001b[1;32m---> 88\u001b[0m \u001b[43mclassifieur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecherche_hyperparametres\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_classifieur\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m nom_final \u001b[38;5;241m=\u001b[39m preprocessing_str \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m vectorization_str \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dimension_reduction \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m classifieur_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m param_classifieur\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_res[nom_final] \u001b[38;5;241m=\u001b[39m classifieur\u001b[38;5;241m.\u001b[39mf1_score()\n",
            "Cell \u001b[1;32mIn[30], line 44\u001b[0m, in \u001b[0;36mClassifieur.recherche_hyperparametres\u001b[1;34m(self, param_grid)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecherche_hyperparametres\u001b[39m(\u001b[38;5;28mself\u001b[39m,param_grid):\n\u001b[0;32m     37\u001b[0m       \u001b[38;5;66;03m# Recherche des meilleurs hyperparamètres avec GridSearchCV\u001b[39;00m\n\u001b[0;32m     38\u001b[0m       grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     39\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier,\n\u001b[0;32m     40\u001b[0m           param_grid,\n\u001b[0;32m     41\u001b[0m           cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,  \u001b[38;5;66;03m# Validation croisée 5-fold\u001b[39;00m\n\u001b[0;32m     42\u001b[0m           scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     43\u001b[0m       )\n\u001b[1;32m---> 44\u001b[0m       \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# tres important de preciser le label posit\u001b[39;00m\n\u001b[0;32m     45\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_ \u001b[38;5;66;03m# Automatiquement garde le meilleur classifieur\u001b[39;00m\n\u001b[0;32m     46\u001b[0m       \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeilleurs hyperparamètres :\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1572\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:995\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    993\u001b[0m     )\n\u001b[1;32m--> 995\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: \nAll the 19440 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3888 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 363, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1012, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 751, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'avis marché attention information contenues lextrait pdf peuvent certains ca pa présenter texte intégral lannonce extraits pdf annonces boamp constituent pa format officiel consulter texte intégral format officiel présent avis cliquez httpswwwboampfrpagesavis qidweb départements publication annonce travaux section identification lacheteur conseil départemental charente nom complet lacheteur siret type numéro national dindentification national didentification angouleme ville code postal non groupement commandes départements publication section communication lien direct document consultation httpswwwmarchespublicsinfompiawsindexcfm fuseactiondematentlogintypedceidm identifiant interne consultation oui intégralité document profil dacheteur non utilisation moyens communication non communément disponibles stéphane quelard nom contact numéro téléphone contact section procédure procédure adaptée ouverte type procédure conditions participation formulaire dc aptitude exercer lactivité professionnelle condition moyens preuve lettre candidature _ habilitation mandataire cotraitants disponible ladresse suivante httpwwweconomiegouvfrdajformulairesdeclarationducandidat formulaire dc déclaration candidat individuel membre groupement disponible ladresse suivante httpwwweconomiegouvfrdajformulairesdeclarationducandidat copie jugements prononcés si candidat redressement judiciaire déclaration appropriée capacité économique financière condition moyens preuve banques preuve dune assurance risques professionnels présentation dune capacités technique professionnelles condition moyens preuve liste travaux exécutés cours cinq dernières années appuyée dattestations bonne exécution travaux plus importants déclaration indiquant loutillage matériel léquipement technique dont candidat dispose réalisation marchés nature déclaration indiquant effectifs moyens annuels candidat limportance personnel dencadrement chacune trois dernières années sans objet technique dachat date heure limite réception plis autorisée présentation offres catalogue électronique non réduction nombre candidats oui possibilité dattribution sans négociation non lacheteur exige présentations variantes valeur technique loffre délai dexécution prix critères dattribution section identification marché aménagement carrefour rd vc bouèges intitulé marché code cpv principal descripteur principal travaux type marché aménagement dun nouveau carrefour entre rd vc description succincte marché bouèges raccordement dun chemin rural fermeture carrefour rd comprenant décapage terre végétale terrassement déblais rabotage scarification chaussée fond forme remblais couche forme couche réglage mise oeuvre bordures a déblais mi remblais création regards pose canalisation têtes buse mise oeuvre gb bbsg création cunettes bétonnées u création fossé engazonnement carrefour rd vc bouèges saintsornin lieu principal dexécution marché durée marché mois non consultation comporte tranche non consultation prévoit réservation tout partie marché non marché alloti section informations complementaires non visite obligatoire présente consultation passée application article autres information complémentaires r r r r code commande publique relatifs dématérialisation procédures passation marchés publics candidats donc possibilité rendant site internet collectivité ladresse httpwwwlacharentefr rubrique marchés public httpmarchespublicslacharentefr consulter avis publicité règlement consultation télécharger dce intégralité poser question dce répondre voie électronique être tenu informés lévolution consultation rectificatif information complémentaires rejet candidature offres date denvoi présent avis publication'\n\n--------------------------------------------------------------------------------\n15552 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 363, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1012, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 751, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'avis attention information contenues lextrait pdf peuvent certains ca pa présenter texte intégral lannonce extraits pdf annonces boamp constituent pa format officiel consulter texte intégral format officiel présent avis cliquez httpswwwboampfrpagesavisqidweb départements publication annonce no i ii iii iv v vi fns simple avis marché section i identification lacheteur nom complet lacheteur cantal habitat type numéro national dindentification siret national didentification ville aurillac code postal groupement commandes non départements publication section communication lien direct document consultation httpswwwmarchespublicsinfompiawsindexcfm fuseactiondematentlogintypedceidm identifiant interne consultation dce_cs_haras intégralité document profil dacheteur oui utilisation moyens communication non communément disponibles non nom contact service marchés publics adresse mail contact numéro téléphone contact section procedure type procédure procédure adaptée ouverte conditions participation aptitude exercer lactivité professionnelle condition moyens preuve formulaire dc lettre candidature _ habilitation mandataire co traitantsdisponible ladresse suivante httpwwweconomiegouvfrdajformulairesdeclarationducandidat formulaire dc déclaration candidat individuel membre groupementdisponible ladresse suivante httpwwweconomiegouvfrdajformulairesdeclarationdu candidatdéclaration lhonneur justifier candidat nentre aucun ca dinterdiction soumissionner capacité économique financière condition moyens preuve déclaration concernant chiffre daffaires global chiffre daffaires concernant prestations objet contrat réalisées cours trois derniers exercices disponibles capacités technique professionnelles condition moyens preuve déclaration indiquant effectifs moyens annuels candidat limportance personnel dencadrement chacune trois dernières annéesdéclaration indiquant loutillage matériel léquipement technique dont candidat dispose réalisation contratliste travaux exécutés cours cinq dernières années appuyée dattestations bonne exécution plus importants montant époque lieu dexécution sils effectués selon règles lart menés bonne fin technique dachat sans objet date heure limite réception plis octobre présentation offres catalogue électronique interdite réduction nombre candidats non possibilité dattribution sans négociation oui lacheteur exige présentations variantes non identification catégories dacheteurs intervenant si accordcadre critères dattribution valeur technique loffre appréciée laide cadre réponse technique pts prix section identification marché intitulé marché travaux daménagement secteur haras aurillac code cpv principal descripteur principal type marché travaux description succincte marché travaux daménagement secteur haras aurillaclopération décomposée phase phase desserte vrd logements construction chemin piéton mois dexécutionphase aménagement extérieur lensemble hors village haras mois dexécutionphase réhabilitation aménagements extérieurs village haras mois dexécutionet dispose pse comme suit pse fourniture pose bordures phases pse réhabilitation aep village haras phase pse eclairage public phases pse aménagement dune aire jeux phase durée prévisionnelle mois dont mois période préparation chantierla consultation fait lobjet dune visite obligatoire dont condition exposées règlement consultationle marché peut faire lobjet négociation lieu principal dexécution marché village haras avenue julien aurillac durée marché mois valeur estimée ht valeur entre consultation comporte tranche non consultation prévoit réservation tout partie marché non marché alloti non mots descripteurs voirie voirie réseaux diver section lots bloc nest pa accessible car lobjet nest pa alloti section informations complementaires visite obligatoire oui détail visite si oui référer larticle rc autres information complémentaires signature électronique nest pa exigéeune garantie financière prévue contrat contrat prévoit versement dune avance obligation constituer garantie première demande contrepartieles prix révisablesle paiement prestations fera respect délai global paiement applicable lacheteurle marché financé fonds propres cantal habitatles voies recours ouvertes candidats suivantes référé précontractuel prévu article code justice administrative cja pouvant être exercé avant signature contrat référé contractuel prévu article cja pouvant être exercé délais prévus larticle r cja recours pleine juridiction ouvert concurrents évincés pouvant être exercé deux mois suivant date laquelle conclusion contrat rendue publique recours contreune décision administrative prévu article r r cja pouvant être exercé mois suivant notification publication décision lorganisme date denvoi présent avis septembre'\n"
          ]
        }
      ],
      "source": [
        "class Projet() :\n",
        "    def __init__(self, ):\n",
        "        with open(\"hyperpamètres_classifieur.jsonl\",\"r\") as file_1:\n",
        "            self.hyper_classifieur = json.load(file_1)\n",
        "        with open(\"hyperparametres_traitement.jsonl\", \"r\") as file_2:\n",
        "            self.hyper_data = json.load(file_2)\n",
        "\n",
        "        ###### CHANGER LES PARAMETRES POUR LES SEQUENCE CI-DESSOUS ######\n",
        "\n",
        "        ## POUR LES DATA ##\n",
        "        #si les deux méthode d'une étape sont False (ex : lemmatisation et racinisation pour l'étape preprocessing )\n",
        "        #Alors l'étape serat toujours égale à null\n",
        "        self.sequence_data = {\n",
        "            \"lemmatisation\": True,\n",
        "            \"racinisation\": False,\n",
        "            \"vectorisation_simple\": False,\n",
        "            \"vectorisation_ponderee\": True,\n",
        "            \"reduction_svd\": False,\n",
        "            \"reduction_nmf\": False\n",
        "        }\n",
        "\n",
        "        ## POUR LES CLASSIFIEURS ##\n",
        "        self.sequence_classifieur = {\n",
        "            Multinomial : False,\n",
        "            Bernoulli : False,\n",
        "            Gaussian : False,\n",
        "            RegressionLogistique : False,\n",
        "            KPlusProchesVoisins : False,\n",
        "            ArbreDeDecision : False,\n",
        "            ForetAleatoire : True\n",
        "        }\n",
        "\n",
        "        ###### CHANGER SOURCE DES DONNEES ######\n",
        "        self.data = \"train.jsonl\"\n",
        "        self.limit_data = 100\n",
        "\n",
        "        ### COMBINAISON DES DATA POSSIBLES ###\n",
        "        self.combo_data = self.generate_data_hyperparam_combinations()\n",
        "        self.nb_combo_data = len(self.combo_data)\n",
        "        print(\"Combo data généré,  nombre : \" + str(self.nb_combo_data))\n",
        "\n",
        "        self.etape_1_sauv = {}\n",
        "        self.etape_1_2_sauv = {}\n",
        "        self.etape_1_2_3_sauv = {}\n",
        "        self.final_res = {}\n",
        "\n",
        "        self.main_loop()\n",
        "\n",
        "    def main_loop(self):\n",
        "\n",
        "        for combinaison in self.combo_data : \n",
        "            print(combinaison)\n",
        "            preprocessing_str = json.dumps(combinaison[\"preprocessing\"], ensure_ascii=False)\n",
        "            vectorization_str = json.dumps(combinaison[\"vectorization\"], ensure_ascii=False)\n",
        "            dimension_reduction = json.dumps(combinaison[\"dimension_reduction\"], ensure_ascii=False)\n",
        "            etape_1_2_str = preprocessing_str + \",\" + vectorization_str\n",
        "            etape_1_2_3_str = preprocessing_str + \",\" + vectorization_str + \",\" + dimension_reduction\n",
        "\n",
        "            if etape_1_2_3_str in self.etape_1_2_3_sauv : \n",
        "                data = self.etape_1_2_3_sauv[etape_1_2_3_str]\n",
        "\n",
        "            if etape_1_2_str in self.etape_1_2_sauv and etape_1_2_3_str not in self.etape_1_2_3_sauv: \n",
        "                data = self.etape_1_2_sauv[etape_1_2_str]\n",
        "                data = self.apply_etape_3(data,combinaison[\"vectorization\"], etape_1_2_str)\n",
        "\n",
        "            if etape_1_2_str not in self.etape_1_2_sauv and preprocessing_str in self.etape_1_sauv and etape_1_2_3_str not in self.etape_1_2_3_sauv: \n",
        "                data = self.etape_1_sauv[vectorization_str]\n",
        "                data = self.apply_etape_2(data, combinaison[\"preprocessing\"], etape_1_2_str)\n",
        "                data = self.apply_etape_3(data,combinaison[\"vectorization\"], etape_1_2_3_str)\n",
        "            \n",
        "            if etape_1_2_str not in self.etape_1_2_sauv and preprocessing_str not in self.etape_1_sauv and etape_1_2_3_str not in self.etape_1_2_3_sauv:\n",
        "                data = self.apply_etape_1(combinaison[\"preprocessing\"], preprocessing_str)\n",
        "\n",
        "                data = self.apply_etape_2(data, combinaison[\"preprocessing\"], etape_1_2_str)\n",
        "                data = self.apply_etape_3(data,combinaison[\"vectorization\"], etape_1_2_3_str)\n",
        "\n",
        "            print(self.etape_1_sauv)\n",
        "            print(self.etape_1_2_sauv)\n",
        "            print(self.etape_1_2_3_sauv)\n",
        "\n",
        "            data = data.get_data()\n",
        "\n",
        "            for classifieur_type, booleen_classifier in self.sequence_classifieur.items() : \n",
        "                if booleen_classifier : \n",
        "                    classifieur = classifieur_type(data)\n",
        "                    classifieur_name = classifieur.__class__.__name__\n",
        "                    param_classifieur = self.hyper_classifieur[classifieur_name]\n",
        "                    classifieur.recherche_hyperparametres(param_classifieur)\n",
        "                    nom_final = preprocessing_str + \",\" + vectorization_str + \",\" + dimension_reduction + \",\" + classifieur_name + \",\" + param_classifieur\n",
        "                    self.final_res[nom_final] = classifieur.f1_score()\n",
        "                    print(\"-\"*40)\n",
        "                    print(nom_final)\n",
        "                    print(\"precision = \" + str(self.final_res[nom_final]))\n",
        "                    print(\"-\"*40)\n",
        "\n",
        "    def apply_etape_1(self,param, nom):\n",
        "\n",
        "        data = Data(limit = self.limit_data)\n",
        "        \n",
        "        if param[\"method\"] == False : return data\n",
        "\n",
        "        if param[\"method\"] == \"racinisation\" : \n",
        "            data.racinisation(ignore_stopwords = param[\"parameters\"][\"ignore_stopwords\"])\n",
        "\n",
        "        if param[\"method\"] == \"lemmatisation\" : \n",
        "            data.lemmatisation(mode = param[\"parameters\"][\"mode\"], overwrite = param[\"parameters\"][\"overwrite\"])\n",
        "\n",
        "        self.etape_1_sauv[nom] = data\n",
        "\n",
        "        return data\n",
        "    \n",
        "\n",
        "    def apply_etape_2(self, data, param, nom):\n",
        "\n",
        "        if param[\"method\"] == False : return data\n",
        "\n",
        "        if param[\"method\"] == \"vectorisation_simple\" : \n",
        "            data.vectorisation_simple(ngram_range = param[\"parameters\"][\"ngram_range\"], max_features = param[\"parameters\"][\"max_features\"])\n",
        "        if param[\"method\"] == \"vectorisation_ponderee\" : \n",
        "            data.vectorisation_ponderee(ngram_range = param[\"parameters\"][\"ngram_range\"], max_features = param[\"parameters\"][\"max_features\"], norm = param[\"parameters\"][\"norm\"])\n",
        "\n",
        "        self.etape_1_2_sauv[nom] = data\n",
        "\n",
        "        return data\n",
        "\n",
        "    def apply_etape_3(self, data, param, nom):\n",
        "\n",
        "        if param[\"method\"] == False : return data\n",
        "\n",
        "        if param[\"method\"] == \"reduction_svd\" : \n",
        "            data.reduction_svd(n_components = param[\"parameters\"][\"n_components\"], n_iter = param[\"parameters\"][\"n_iter\"])\n",
        "        \n",
        "        if param[\"method\"] == \"reduction_nmf\" : \n",
        "            data.reduction_nmf(n_components = param[\"parameters\"][\"n_components\"], init = param[\"parameters\"][\"init\"])\n",
        "        \n",
        "        self.etape_1_2_3_sauv[nom + \",\" + json.dumps(param, ensure_ascii=False)] = data\n",
        "\n",
        "        return data\n",
        "    \n",
        "\n",
        "    def generate_data_hyperparam_combinations(self):\n",
        "        all_combinations = []\n",
        "\n",
        "        preprocessing_steps = self.hyper_data.get(\"preprocessing\", {})\n",
        "        vectorization_steps = self.hyper_data.get(\"vectorization\", {})\n",
        "        reduction_steps = self.hyper_data.get(\"dimension_reduction\", {})\n",
        "\n",
        "        # Check if all methods for an entire step are disabled\n",
        "        if not any(self.sequence_data.get(step, False) for step in preprocessing_steps):\n",
        "            preprocessing_steps = {}\n",
        "        if not any(self.sequence_data.get(step, False) for step in vectorization_steps):\n",
        "            vectorization_steps = {}\n",
        "        if not any(self.sequence_data.get(step, False) for step in reduction_steps):\n",
        "            reduction_steps = {}\n",
        "\n",
        "        preprocessing_combos = self._generate_combos_data(preprocessing_steps, \"preprocessing\")\n",
        "        vectorization_combos = self._generate_combos_data(vectorization_steps, \"vectorization\")\n",
        "        reduction_combos = self._generate_combos_data(reduction_steps, \"dimension_reduction\")\n",
        "\n",
        "        if not preprocessing_combos:\n",
        "            preprocessing_combos = [None]\n",
        "        if not vectorization_combos:\n",
        "            vectorization_combos = [None]\n",
        "        if not reduction_combos:\n",
        "            reduction_combos = [None]\n",
        "\n",
        "        for pre in preprocessing_combos:\n",
        "            for vec in vectorization_combos:\n",
        "                for red in reduction_combos:\n",
        "                    all_combinations.append({\n",
        "                        \"preprocessing\": pre,\n",
        "                        \"vectorization\": vec,\n",
        "                        \"dimension_reduction\": red\n",
        "                    })\n",
        "\n",
        "        return all_combinations\n",
        "\n",
        "    def _generate_combos_data(self, steps, step_type):\n",
        "        \"\"\"Generate all valid combinations for a specific step type.\"\"\"\n",
        "        step_combinations = []\n",
        "        for step_name, params in steps.items():\n",
        "            if self.sequence_data.get(step_name, False):\n",
        "                param_combos = self._generate_param_data_combinations(params)\n",
        "                for combo in param_combos:\n",
        "                    step_combinations.append({\"method\": step_name, \"parameters\": combo})\n",
        "        return step_combinations\n",
        "\n",
        "    def _generate_param_data_combinations(self, params):\n",
        "        \"\"\"Generate all combinations of parameters for a given step.\"\"\"\n",
        "        if not params:\n",
        "            return [{}]\n",
        "\n",
        "        keys, values = zip(*params.items())\n",
        "        param_combos = [dict(zip(keys, combination)) for combination in product(*values)]\n",
        "        return param_combos\n",
        "\n",
        "\n",
        "\n",
        "projet = Projet()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " A PARTIR DE CE MOMENT LA C'EST DES TEST --------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGn5EGEsRLKG"
      },
      "source": [
        "TEST COMBO DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIOwUOU6RLKH",
        "outputId": "6c0aaa87-e555-41b2-b003-e1588dec13a7"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "class DataPipeline:\n",
        "    def __init__(self):\n",
        "        self.sequence_data = {\n",
        "            \"racinisation\": True,\n",
        "            \"lemmatisation\": True,\n",
        "            \"vectorisation_simple\": True,\n",
        "            \"vectorisation_ponderee\": True,\n",
        "            \"reduction_svd\": True,\n",
        "            \"reduction_nmf\": True\n",
        "        }\n",
        "\n",
        "        self.hyper_data = {\n",
        "            \"preprocessing\": {\n",
        "                \"racinisation\": {\n",
        "                    \"ignore_stopwords\": [True, False]\n",
        "                },\n",
        "                \"lemmatisation\": {\n",
        "                    \"mode\": [\"lookup\", \"rule\"],\n",
        "                    \"overwrite\": [True, False]\n",
        "                }\n",
        "            },\n",
        "            \"vectorization\": {\n",
        "                \"vectorisation_simple\": {\n",
        "                    \"ngram_range\": [[1, 1], [1, 2]],\n",
        "                    \"max_features\": [None, 5000]\n",
        "                },\n",
        "                \"vectorisation_ponderee\": {\n",
        "                    \"ngram_range\": [[1, 1], [1, 2], [2, 2]],\n",
        "                    \"max_features\": [None, 5000],\n",
        "                    \"norm\": [\"l2\", \"l1\"]\n",
        "                }\n",
        "            },\n",
        "            \"dimension_reduction\": {\n",
        "                \"reduction_svd\": {\n",
        "                    \"n_components\": [50, 100, 200],\n",
        "                    \"n_iter\": [5, 10, 15]\n",
        "                },\n",
        "                \"reduction_nmf\": {\n",
        "                    \"n_components\": [50, 100, 200],\n",
        "                    \"init\": [\"random\", \"nndsvd\", \"use_idf\"]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def generate_data_hyperparam_combinations(self):\n",
        "        all_combinations = []\n",
        "\n",
        "        preprocessing_steps = self.hyper_data.get(\"preprocessing\", {})\n",
        "        vectorization_steps = self.hyper_data.get(\"vectorization\", {})\n",
        "        reduction_steps = self.hyper_data.get(\"dimension_reduction\", {})\n",
        "\n",
        "        # Check if all methods for an entire step are disabled\n",
        "        if not any(self.sequence_data.get(step, False) for step in preprocessing_steps):\n",
        "            preprocessing_steps = {}\n",
        "        if not any(self.sequence_data.get(step, False) for step in vectorization_steps):\n",
        "            vectorization_steps = {}\n",
        "        if not any(self.sequence_data.get(step, False) for step in reduction_steps):\n",
        "            reduction_steps = {}\n",
        "\n",
        "        preprocessing_combos = self._generate_combos_data(preprocessing_steps, \"preprocessing\")\n",
        "        vectorization_combos = self._generate_combos_data(vectorization_steps, \"vectorization\")\n",
        "        reduction_combos = self._generate_combos_data(reduction_steps, \"dimension_reduction\")\n",
        "\n",
        "        if not preprocessing_combos:\n",
        "            preprocessing_combos = [None]\n",
        "        if not vectorization_combos:\n",
        "            vectorization_combos = [None]\n",
        "        if not reduction_combos:\n",
        "            reduction_combos = [None]\n",
        "\n",
        "        for pre in preprocessing_combos:\n",
        "            for vec in vectorization_combos:\n",
        "                for red in reduction_combos:\n",
        "                    all_combinations.append({\n",
        "                        \"preprocessing\": pre,\n",
        "                        \"vectorization\": vec,\n",
        "                        \"dimension_reduction\": red\n",
        "                    })\n",
        "\n",
        "        return all_combinations\n",
        "\n",
        "    def _generate_combos_data(self, steps, step_type):\n",
        "        \"\"\"Generate all valid combinations for a specific step type.\"\"\"\n",
        "        step_combinations = []\n",
        "        for step_name, params in steps.items():\n",
        "            if self.sequence_data.get(step_name, False):\n",
        "                param_combos = self._generate_param_data_combinations(params)\n",
        "                for combo in param_combos:\n",
        "                    step_combinations.append({\"method\": step_name, \"parameters\": combo})\n",
        "        return step_combinations\n",
        "\n",
        "    def _generate_param_data_combinations(self, params):\n",
        "        \"\"\"Generate all combinations of parameters for a given step.\"\"\"\n",
        "        if not params:\n",
        "            return [{}]\n",
        "\n",
        "        keys, values = zip(*params.items())\n",
        "        param_combos = [dict(zip(keys, combination)) for combination in product(*values)]\n",
        "        return param_combos\n",
        "\n",
        "# Example usage\n",
        "pipeline = DataPipeline()\n",
        "result = pipeline.generate_data_hyperparam_combinations()\n",
        "\n",
        "# Output the result in a readable JSON format\n",
        "import json\n",
        "print(json.dumps(result, indent=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeF-Q305_nzb"
      },
      "source": [
        "Fonction ultra débile qui teste tous les classifieurs, pré traitements, vectorisation et réduction afin d'avoir le meilleurs taux de réussite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHJ6FYyH4au6",
        "outputId": "d6e185be-6301-46b1-e451-47be3dace599"
      },
      "outputs": [],
      "source": [
        "def possibilite():\n",
        "    results = []  # Liste pour stocker les résultats\n",
        "\n",
        "    with open(\"hyperpamètres_classifieur.jsonl\",\"r\") as file:\n",
        "        hyper_classifieur = json.load(file)\n",
        "\n",
        "    for x in [\"lemmatisation\", \"racinisation\",\"rien\"]:\n",
        "            data = Data(limit=100)\n",
        "            if x == \"lemmatisation\" :\n",
        "                for mode in [\"lookup\", \"rule\"]:\n",
        "                  for overit in [True, False]:\n",
        "                    data.lemmatisation(mode,overit)\n",
        "                    print(\"etape 1\")\n",
        "                    data.vectorisation_ponderee()\n",
        "                    print(\"etape 2\")\n",
        "                    for y in [\"nmf\",\"svd\" ]:\n",
        "                      if y == \"nmf\":\n",
        "                          data.reduction_nmf(n_components = 100)\n",
        "                          print(\"etape 3\")\n",
        "                          z = \"nmf\"\n",
        "                      else:\n",
        "                          data.reduction_svd(n_components = 100)\n",
        "                          print(\"etape 4\")\n",
        "                          z = \"svd\"\n",
        "#Meilleurs hyperparamètres : {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
        "                      # Parcours des classifieurs\n",
        "                      for i in [ForetAleatoire]:\n",
        "                          classifieur = i(data.get_data())\n",
        "                          classifieur.recherche_hyperparametres()\n",
        "                          taux_reussite = classifieur.taux_reussite()\n",
        "                          f1_score = classifieur.f1_score()\n",
        "\n",
        "                          # Ajout des résultats à la liste sous forme de tuple\n",
        "                          results.append(x, mode, overit, y, z, i.__name__, taux_reussite,f1_score)\n",
        "                          print(results[len(results)])\n",
        "\n",
        "\n",
        "            elif x == \"racinisation\" :\n",
        "              for stop_word in [True, False]:\n",
        "                data.racinisation(stop_word)\n",
        "                data.vectorisation_ponderee()\n",
        "                for y in [\"vectorisation_ponderee\",\"vectorisation_simple\"]:\n",
        "                      if y == \"vectorisation_ponderee\":\n",
        "                          data.reduction_nmf(n_components = 100)\n",
        "                          z = \"nmf\"\n",
        "                      else:\n",
        "                          data.reduction_svd(n_components = 100)\n",
        "                          z = \"svd\"\n",
        "\n",
        "                      # Parcours des classifieurs\n",
        "                      for i in [ForetAleatoire]:\n",
        "                          classifieur = i(data.get_data())\n",
        "                          classifieur.recherche_hyperparametres()\n",
        "                          taux_reussite = classifieur.taux_reussite()\n",
        "                          f1_score = classifieur.f1_score()\n",
        "\n",
        "                          # Ajout des résultats à la liste sous forme de tuple\n",
        "                          results.append((x, mode, overit, y, z, i.__name__, taux_reussite,f1_score))\n",
        "                          print(results[len(results)])\n",
        "\n",
        "    # Tri des résultats par taux de réussite (du plus élevé au plus faible)\n",
        "    results_sorted = sorted(results, key=lambda x: x[6], reverse=True)\n",
        "\n",
        "    # Affichage des résultats triés\n",
        "    for res in results_sorted:\n",
        "        print(f\"{res[0]} - {res[1]} - {res[2]} - {res[3]} - {res[4]} - {res[5]} : {res[6]}\")\n",
        "\n",
        "possibilite()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd2b2L2IAW4L"
      },
      "source": [
        "Exemple d'utilisation en temps normal pour l'optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FL1rQpFAdBF",
        "outputId": "e1ef638f-ab60-4fb7-c2d4-ad488d1ebfa0"
      },
      "outputs": [],
      "source": [
        "data = Data()\n",
        "data.lemmatisation()\n",
        "data.vectorisation_ponderee()\n",
        "data.reduction_nmf()\n",
        "data = data.get_data()\n",
        "\n",
        "foret = ForetAleatoire(data)\n",
        "foret.entrainement()\n",
        "print(foret.f1_score())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXVYhTmDL4Uu"
      },
      "source": [
        "Optimisation de la foret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYIVbtY9L4DU",
        "outputId": "1a26613d-2934-4917-e741-a5fd0a976ea3"
      },
      "outputs": [],
      "source": [
        "#Donne les meilleurs hyperparametre de la foret\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "data = Data(limit = 50)\n",
        "print(\"etape 1\")\n",
        "data.lemmatisation()\n",
        "print(\"etape 2\")\n",
        "data.vectorisation_ponderee()\n",
        "print(\"etape 3\")\n",
        "data.reduction_nmf()\n",
        "print(\"etape 4\")\n",
        "data = data.get_data()\n",
        "print(\"etape 5\")\n",
        "\n",
        "foret = ForetAleatoire(data)\n",
        "print(\"etape 6\")\n",
        "foret.recherche_hyperparametres(param_grid)\n",
        "print(\"etape 7\")\n",
        "foret.f1_score()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
