{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SAE Data Mining : Colin David Ruth et Thibault\n",
        "        \n",
        "### Contexte\n",
        "**Dans le cadre de cette SAE nous nous sommes appuyés sur le TP noté précédemment réalisé.**  \n",
        "Nous avons constaté que le projet précédent impliquait de nombreux copier-coller ce qui limitait la flexibilité du code. Cela nous a amenés à repenser la structure du projet pour **faciliter les tests de différentes hypothèses et configurations**.  \n",
        "Notre objectif était donc de concevoir une **architecture de code à la fois flexible et compréhensible**.\n",
        "\n",
        "---\n",
        "        \n",
        "### Approche Adoptée\n",
        "Pour répondre à ces besoins nous avons opté pour une **approche orientée objet** permettant de mieux répartir les tâches et de travailler de manière indépendante.Nous avons structuré le projet autour de **quatre classes principales** :  \n",
        "        \n",
        "1. **Data**: Gère l'importation la transformation et la vectorisation des données.\n",
        "        \n",
        "2. **Classifieur**: Regroupe les fonctionnalités communes à tous les classifieurs.\n",
        "        \n",
        "3. **Les différents classifieurs**: Héritent de la classe *Classifieur* pour implémenter les spécificités de chaque modèle.\n",
        "        \n",
        "4. **Projet**: Centralise les tests en explorant les hyperparamètres de la classe *Data* ainsi que ceux des différents classifieurs.\n",
        "        \n",
        "---\n",
        "        \n",
        "### Bénéfices\n",
        "Cette structure nous a permis :  \n",
        "- D’optimiser l'organisation du projet\n",
        "- De faciliter l'exploration et l’expérimentation\n",
        "- Et de rendre le code plus modulaire et accessible pour toute l'équipe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAeNBJIH4eo7"
      },
      "source": [
        "## Import des bibliothèques nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXJFT3hINTdG",
        "outputId": "1fbc9125-d7af-4241-ecd5-93fa5859be1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Brasi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Brasi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importations standard\n",
        "import json\n",
        "import re\n",
        "import copy\n",
        "from datetime import datetime\n",
        "import os\n",
        "import seaborn as sns\n",
        "\n",
        "# Importations de bibliothèques tierces\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product\n",
        "\n",
        "# Importations des modules sklearn\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD, NMF\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Importations nltk\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Vérifier et télécharger les ressources nltk si nécessaires\n",
        "nltk_resources = ['stopwords', 'punkt', 'wordnet']\n",
        "for resource in nltk_resources:\n",
        "    try:\n",
        "        nltk.data.find(f'tokenizers/{resource}' if resource == 'punkt' else f'corpora/{resource}')\n",
        "    except LookupError:\n",
        "        nltk.download(resource)\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc4hk45n4kjt"
      },
      "source": [
        "## Classe Data pour tous les prétraitements, la vectorisation et la réduction de la matrice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "-JQ20_eh4J4b"
      },
      "outputs": [],
      "source": [
        "class Data:\n",
        "    def __init__(self, data = \"train.jsonl\", test= False, limit=5000, language=\"french\", text_column=\"texte_annonce\"):\n",
        "        if test==True:\n",
        "            self.data = (pd.read_json(\"test.jsonl\", lines=True)[[\"texte_annonce\",\"OGC_FID\"]])\n",
        "        else:\n",
        "            self.data = (pd.read_json(\"train.jsonl\", lines=True, encoding=\"UTF-8\")[[\"texte_annonce\", \"cal_réponse_signalement\"]]).iloc[:limit]\n",
        "            self.data[\"cal_réponse_signalement\"] = self.data[\"cal_réponse_signalement\"].map({\"Pris en compte\": 0,\"Rejete (hors specs)\": 1})\n",
        "        self.text_column = text_column\n",
        "        self.language = language\n",
        "\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.data\n",
        "\n",
        "    def supprimer_stopwords(self):\n",
        "        stop_words = stopwords.words(self.language)\n",
        "\n",
        "        def nettoyer_texte(texte):\n",
        "            texte = re.sub(r'[^\\w\\s]', '', texte)  # Retirer la ponctuation\n",
        "            texte = re.sub(r'\\d+', '', texte)      # Retirer les chiffres\n",
        "            tokens = nltk.word_tokenize(texte.lower())\n",
        "            return ' '.join([word for word in tokens if word not in stop_words])\n",
        "\n",
        "        # Appliquer la fonction de nettoyage à la colonne texte\n",
        "        self.data[self.text_column] = self.data[self.text_column].apply(nettoyer_texte)\n",
        "\n",
        "    def rien(self, x):\n",
        "        pass\n",
        "\n",
        "    def racinisation(self, ignore_stopwords=None):\n",
        "        stemmer = SnowballStemmer(self.language)\n",
        "        self.data[self.text_column] = self.data[self.text_column].apply(lambda x: ' '.join([stemmer.stem(word) for word in str(x).split()]))\n",
        "        self.supprimer_stopwords()\n",
        "\n",
        "    def lemmatisation(self, mode=None, overwrite=None):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        if mode:\n",
        "            lemmatizer.mode = mode\n",
        "        if overwrite:\n",
        "            lemmatizer.overwrite = overwrite\n",
        "        self.data[self.text_column] = self.data[self.text_column].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in str(x).split()]))\n",
        "        self.supprimer_stopwords()\n",
        "\n",
        "    def vectorisation_simple(self, ngram_range=None, max_features=None):\n",
        "        vectorizer = CountVectorizer()\n",
        "        if ngram_range:\n",
        "            vectorizer.ngram_range = ngram_range\n",
        "        if max_features:\n",
        "            vectorizer.max_features = max_features\n",
        "        vect_data = vectorizer.fit_transform(self.data[self.text_column])\n",
        "        vect_df = pd.DataFrame(vect_data.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "        self.data = pd.concat([vect_df, self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def vectorisation_ponderee(self, ngram_range=None, max_features=None, norm=None):\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        if ngram_range:\n",
        "            vectorizer.ngram_range = ngram_range\n",
        "        if max_features:\n",
        "            vectorizer.max_features = max_features\n",
        "        if norm:\n",
        "            vectorizer.norm = norm\n",
        "        vect_data = vectorizer.fit_transform(self.data[self.text_column])\n",
        "        vect_df = pd.DataFrame(vect_data.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "        self.data = pd.concat([vect_df, self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def reduction_svd(self, n_components=50, n_iter=None):\n",
        "        svd = TruncatedSVD()           \n",
        "        if n_iter:\n",
        "            svd.n_iter = n_iter\n",
        "        svd = TruncatedSVD(n_components=n_components)\n",
        "        svd_result = svd.fit_transform(self.data.iloc[:, :-1])\n",
        "        self.data = pd.concat([pd.DataFrame(svd_result), self.data.iloc[:, -1]], axis=1)\n",
        "\n",
        "    def reduction_nmf(self, n_components=50, init=None):\n",
        "        nmf = NMF(n_components=n_components)\n",
        "        if init:\n",
        "            nmf.init = init\n",
        "        nmf_result = nmf.fit_transform(self.data.iloc[:, :-1])\n",
        "        self.data = pd.concat([pd.DataFrame(nmf_result), self.data.iloc[:, -1]], axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFjpIvyd_Y5S"
      },
      "source": [
        "## Classe de base des classifieurs permettant l'héritage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "RA_Ne8kbpOGD"
      },
      "outputs": [],
      "source": [
        "class Classifieur:\n",
        "    def __init__(self, data):\n",
        "        # Chargement et découpage des données\n",
        "        self.chargement(data)\n",
        "        self.decoupage()\n",
        "\n",
        "    def chargement(self, data):\n",
        "        # Chargement des données et séparation X, y\n",
        "        self.data = data\n",
        "        self.X = data.iloc[:, :-1]  # Toutes les colonnes sauf la dernière (X)\n",
        "        self.y = data.iloc[:, -1]   # Dernière colonne (y)\n",
        "\n",
        "    def decoupage(self):\n",
        "        # Découpage en jeu d'entrainement et test\n",
        "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.X, self.y, test_size=0.6, random_state=0)\n",
        "\n",
        "    def entrainement(self):\n",
        "        pass\n",
        "\n",
        "    def rien(self):\n",
        "        pass\n",
        "\n",
        "    def taux_reussite(self):\n",
        "        # Calcul du taux de réussite\n",
        "        y_pred = self.classifier.predict(self.X_test)\n",
        "        accuracy = accuracy_score(self.Y_test, y_pred)\n",
        "        return f\"{accuracy:.3f}\"\n",
        "\n",
        "    def f1_score(self, graph = False):\n",
        "        # Calcul du F1-score\n",
        "        from sklearn.metrics import f1_score\n",
        "        import seaborn as sns\n",
        "        y_pred = self.classifier.predict(self.X_test)\n",
        "        f1 = f1_score(self.Y_test, y_pred, average='weighted')\n",
        "\n",
        "        if graph:\n",
        "            cm = confusion_matrix(self.Y_test, y_pred)\n",
        "            \n",
        "            # Affichage avec seaborn\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=self.classifier.classes_, yticklabels=self.classifier.classes_)\n",
        "            plt.title('Matrice de Confusion')\n",
        "            plt.xlabel('Prédictions')\n",
        "            plt.ylabel('Vérités')\n",
        "            plt.show()\n",
        "\n",
        "        return f\"{f1:.3f}\"\n",
        "\n",
        "    def recherche_hyperparametres(self,param_grid):\n",
        "          # Recherche des meilleurs hyperparamètres avec GridSearchCV\n",
        "          grid_search = GridSearchCV(\n",
        "              self.classifier,\n",
        "              param_grid,\n",
        "              cv=5,  # Validation croisée 5-fold\n",
        "              scoring='f1',\n",
        "          )\n",
        "          grid_search.fit(self.X_train, self.Y_train) # tres important de preciser le label posit\n",
        "          self.classifier = grid_search.best_estimator_ # Automatiquement garde le meilleur classifieur\n",
        "          return grid_search.best_params_\n",
        "    \n",
        "    def predict(self, data):\n",
        "        X = data.iloc[:, :-1]  \n",
        "        ogc = data.iloc[:, -1] \n",
        "\n",
        "        y_pred = self.classifier.predict(X)  \n",
        "        y_pred = pd.Series(y_pred, name=\"cal_réponse_signalement\")\n",
        "        y_pred = y_pred.map({0: \"Pris en compte\", 1: \"Rejeté (hors specs)\"})\n",
        "\n",
        "        result = pd.concat([ogc, y_pred], axis=1)\n",
        "        result.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "nAx2MGzSOn_S"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "\n",
        "class Multinomial(Classifieur):\n",
        "    def __init__(self, data, alpha = None, fit_prior = None):\n",
        "        super().__init__(data)\n",
        "        self.classifier = MultinomialNB()\n",
        "        if alpha : self.classifier.alpha = alpha\n",
        "        if fit_prior : self.classifier.fit_prior = fit_prior\n",
        "\n",
        "    def entrainement(self):\n",
        "        # Entraînement du modèle avec les données\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "Mbtkcyg7njLK"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "\n",
        "class Bernoulli(Classifieur):\n",
        "    def __init__(self, data, alpha = None, fit_prior = None, binarize = None):\n",
        "        super().__init__(data)\n",
        "\n",
        "        self.classifier = BernoulliNB()\n",
        "        if alpha : self.classifier.alpha = alpha\n",
        "        if fit_prior : self.classifier.fit_prior = fit_prior\n",
        "        if binarize : self.classifier.binarize = binarize\n",
        "\n",
        "\n",
        "    def entrainement(self):\n",
        "        # Entraînement du modèle avec les données\n",
        "        self.classifier.fit(self.X_train, self.Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "5Kpxa-J6njnS"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "class Gaussian(Classifieur):\n",
        "    def __init__(self, data, var_smoothing = None):\n",
        "        super().__init__(data)\n",
        "        self.classifier = GaussianNB()\n",
        "        if var_smoothing : self.classifier.var_smoothing = var_smoothing\n",
        "\n",
        "    def entrainement(self):\n",
        "        # Entraînement du modèle avec les données\n",
        "        self.classifier.fit(self.X_train, self.Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "8fqw-IaPRTQ0"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "class RegressionLogistique(Classifieur):\n",
        "    def __init__(self, data, penalty = None, C = None, solver = None, max_iter = None, l1_ratio = None, random_state = None):\n",
        "        super().__init__(data)\n",
        "        self.classifier = LogisticRegression()\n",
        "        if penalty : self.classifier.penalty = penalty\n",
        "        if C : self.classifier.C = C\n",
        "        if solver : self.classifier.solver = solver\n",
        "        if max_iter : self.classifier.max_iter = max_iter\n",
        "        if l1_ratio : self.classifier.l1_ratio\n",
        "        if random_state : self.classifier.random_state = random_state\n",
        "\n",
        "    def entrainement(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "pE4gII77RjTM"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "class KPlusProchesVoisins(Classifieur):\n",
        "    def __init__(self, data, n_neighbors = 3, weights = None, algorithm = None, p = None):\n",
        "        super().__init__(data)\n",
        "\n",
        "        self.classifier = KNeighborsClassifier()\n",
        "        if n_neighbors : self.classifier.n_neighbors = n_neighbors\n",
        "        if weights : self.classifier.weights = weights\n",
        "        if algorithm : self.classifier.algorithm = algorithm\n",
        "        if p : self.classifier.p = p\n",
        "\n",
        "    def entrainement(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "RPUV2vAXRx4G"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "class ArbreDeDecision(Classifieur):\n",
        "    def __init__(self, data, criterion = None, max_depth = None, min_samples_split = None, min_samples_leaf = None, max_features = None, ccp_alpha = None, random_state = None):\n",
        "        super().__init__(data)\n",
        "        self.classifier = DecisionTreeClassifier()\n",
        "        if criterion : self.classifier.criterion = criterion\n",
        "        if max_depth : self.classifier.max_depth = max_depth\n",
        "        if min_samples_split : self.classifier.min_samples_split = min_samples_split\n",
        "        if min_samples_leaf : self.classifier.min_samples_leaf = min_samples_leaf\n",
        "        if max_features : self.classifier.max_features = max_features\n",
        "        if ccp_alpha : self.classifier.ccp_alpha = ccp_alpha\n",
        "        if random_state : self.classifier.random_state = random_state\n",
        "\n",
        "    def entrainement(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "t-mIMWs9R_AW"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "class ForetAleatoire(Classifieur):\n",
        "    def __init__(self, data, n_estimators = 100, criterion = None, max_depth = None, min_samples_split = None, min_samples_leaf = None, max_features = None, bootstrap = None, ccp_alpha = None, random_state = 0 ):\n",
        "        super().__init__(data)\n",
        "        self.classifier = RandomForestClassifier()\n",
        "        if n_estimators : self.classifier.n_estimators = n_estimators\n",
        "        if criterion : self.classifier.criterion = criterion\n",
        "        if max_depth : self.classifier.max_depth = max_depth\n",
        "        if min_samples_split : self.classifier.min_samples_split = min_samples_split\n",
        "        if min_samples_leaf : self.classifier.min_samples_leaf = min_samples_leaf\n",
        "        if max_features : self.classifier.max_features = max_features\n",
        "        if bootstrap : self.classifier.bootstrap = bootstrap\n",
        "        if ccp_alpha : self.classifier.ccp_alpha = ccp_alpha\n",
        "        if random_state : self.classifier.random_state = random_state\n",
        "\n",
        "    def entrainement(self):\n",
        "        self.classifier.fit(self.X_train, self.Y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classe Projet intégrant les classes précédentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZZimGH9RLKG",
        "outputId": "148e89cb-75dd-4a28-b591-473eeeb73313"
      },
      "outputs": [],
      "source": [
        "class Projet() :\n",
        "    def __init__(self, sequence_data, sequence_classifieur, data_json = \"train.jsonl\", limit_data = 100, json_resultat = \"json_résultat\"):\n",
        "\n",
        "        self.valider_nomenclature(sequence_data, sequence_classifieur)\n",
        "\n",
        "        \n",
        "\n",
        "        self.sequence_data = sequence_data\n",
        "        self.sequence_classifieur = sequence_classifieur\n",
        "        self.data_json = data_json \n",
        "        self.limit_data = limit_data\n",
        "        self.json_resultat = json_resultat\n",
        "\n",
        "\n",
        "        self.verif_json_reslutat_name()\n",
        "\n",
        "\n",
        "        with open(\"hyperpamètres_classifieur.jsonl\",\"r\") as file_1:\n",
        "            self.hyper_classifieur = json.load(file_1)\n",
        "        with open(\"hyperparametres_traitement.jsonl\", \"r\") as file_2:\n",
        "            self.hyper_data = json.load(file_2)\n",
        "\n",
        "\n",
        "        if self.sequence_data[\"vectorisation_simple\"] == False and self.sequence_data[\"vectorisation_ponderee\"] == False : self.sequence_data[\"vectorisation_simple\"] = True\n",
        "        \n",
        "\n",
        "        ### COMBINAISON DES DATA POSSIBLES ###\n",
        "        self.combo_data = self.generate_data_hyperparam_combinations()\n",
        "        self.nb_combo_data = len(self.combo_data)\n",
        "\n",
        "        print(self.nb_combo_data)\n",
        "\n",
        "        self.etape_1_sauv = {}\n",
        "        self.etape_1_2_sauv = {}\n",
        "        self.etape_1_2_3_sauv = {}\n",
        "        self.final_res = {}\n",
        "\n",
        "        self.main_loop()\n",
        "\n",
        "    def main_loop(self):\n",
        "\n",
        "        for combinaison in self.combo_data : \n",
        "            self.data = Data(data=self.data_json, limit = self.limit_data)\n",
        "            preprocessing_str = json.dumps(combinaison[\"preprocessing\"], ensure_ascii=False)\n",
        "            vectorization_str = json.dumps(combinaison[\"vectorization\"], ensure_ascii=False)\n",
        "            dimension_reduction = json.dumps(combinaison[\"dimension_reduction\"], ensure_ascii=False)\n",
        "            etape_1_2_str = preprocessing_str + \",\" + vectorization_str\n",
        "            etape_1_2_3_str = preprocessing_str + \",\" + vectorization_str + \",\" + dimension_reduction\n",
        "\n",
        "\n",
        "            if etape_1_2_3_str in self.etape_1_2_3_sauv : \n",
        "                self.data = copy.deepcopy(self.etape_1_2_3_sauv[etape_1_2_3_str])\n",
        "\n",
        "\n",
        "\n",
        "            if etape_1_2_str in self.etape_1_2_sauv and etape_1_2_3_str not in self.etape_1_2_3_sauv: \n",
        "                self.data = copy.deepcopy(self.etape_1_2_sauv[etape_1_2_str])\n",
        "                print(self.data.get_data())\n",
        "                self.apply_etape_3(combinaison[\"dimension_reduction\"], etape_1_2_3_str)\n",
        "\n",
        "            if (etape_1_2_str not in self.etape_1_2_sauv) and (preprocessing_str in self.etape_1_sauv) and (etape_1_2_3_str not in self.etape_1_2_3_sauv): \n",
        "                self.data = copy.deepcopy(self.etape_1_sauv[preprocessing_str])\n",
        "                self.apply_etape_2(combinaison[\"vectorization\"], etape_1_2_str)\n",
        "                self.apply_etape_3(combinaison[\"dimension_reduction\"], etape_1_2_3_str)\n",
        "            \n",
        "            if etape_1_2_str not in self.etape_1_2_sauv and preprocessing_str not in self.etape_1_sauv and etape_1_2_3_str not in self.etape_1_2_3_sauv:\n",
        "                self.apply_etape_1(combinaison[\"preprocessing\"], preprocessing_str)\n",
        "                self.apply_etape_2(combinaison[\"vectorization\"], etape_1_2_str)\n",
        "                self.apply_etape_3(combinaison[\"dimension_reduction\"], etape_1_2_3_str)\n",
        "\n",
        "\n",
        "            self.data = self.data.get_data()\n",
        "\n",
        "            for classifieur_type, booleen_classifier in self.sequence_classifieur.items() : \n",
        "                if booleen_classifier : \n",
        "                    classifieur = classifieur_type(self.data)\n",
        "                    classifieur_name = classifieur.__class__.__name__\n",
        "                    param_classifieur = self.hyper_classifieur[classifieur_name]\n",
        "                    best_param = classifieur.recherche_hyperparametres(param_classifieur)\n",
        "                    self.mise_en_forme_final_export_json(combinaison[\"preprocessing\"], combinaison[\"vectorization\"], combinaison[\"dimension_reduction\"], classifieur_name, best_param, classifieur.f1_score())\n",
        "                    nom_final = preprocessing_str + \",\" + vectorization_str + \",\" + dimension_reduction + \",\" + classifieur_name + \",\" + json.dumps(param_classifieur, ensure_ascii=False)\n",
        "                    self.final_res[nom_final] = classifieur.f1_score()\n",
        "\n",
        "\n",
        "\n",
        "    def mise_en_forme_final_export_json(self, preprocessing, vectorization, dimension_reduction, classifieur_name, best_param, f1_score):\n",
        "        res = {\n",
        "            \"preprocessing\" : preprocessing,\n",
        "            \"vectorization\" : vectorization, \n",
        "            \"dimension_reduction\" : dimension_reduction,\n",
        "            \"classifieur_name\" : classifieur_name,\n",
        "            \"best_param\" : best_param, \n",
        "            \"f1_score\" : f1_score\n",
        "        }\n",
        "\n",
        "\n",
        "        if not os.path.exists(self.json_resultat):\n",
        "            with open(self.json_resultat, 'w', encoding='utf-8') as fichier:\n",
        "                json.dump([], fichier)\n",
        "\n",
        "        with open(self.json_resultat, 'r', encoding='utf-8') as fichier:\n",
        "                data_json_resultat = json.load(fichier)\n",
        "\n",
        "        data_json_resultat.append(res)\n",
        "\n",
        "        with open(self.json_resultat, 'w', encoding='utf-8') as fichier:\n",
        "            json.dump(data_json_resultat, fichier, indent=4, ensure_ascii=False)\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def verif_json_reslutat_name(self):\n",
        "        if os.path.exists(self.json_resultat):\n",
        "            date_heure_actuelle = datetime.now()\n",
        "            date_heure_chaine = date_heure_actuelle.strftime(\"%d-%m %H:%M:%S\")\n",
        "            date_heure_transformee = date_heure_chaine.replace(\":\", \"-\").replace(\" \", \"-\")\n",
        "            self.json_resultat = self.json_resultat + \" \" + date_heure_transformee\n",
        "            return \n",
        "        else:\n",
        "            return True  \n",
        "\n",
        "\n",
        "    def apply_etape_1(self,param, nom):\n",
        "\n",
        "        if param == None : return \n",
        "\n",
        "        if param[\"method\"] == \"racinisation\" : \n",
        "            self.data.racinisation(ignore_stopwords = param[\"parameters\"][\"ignore_stopwords\"])\n",
        "\n",
        "        if param[\"method\"] == \"lemmatisation\" : \n",
        "            self.data.lemmatisation(mode = param[\"parameters\"][\"mode\"], overwrite = param[\"parameters\"][\"overwrite\"])\n",
        "\n",
        "        self.etape_1_sauv[nom] = copy.deepcopy(self.data)\n",
        "\n",
        "    def apply_etape_2(self,  param, nom):\n",
        "        if param == None : return \n",
        "\n",
        "        if param[\"method\"] == \"vectorisation_simple\" :\n",
        "            self.data.vectorisation_simple(ngram_range = tuple(param[\"parameters\"][\"ngram_range\"]), max_features = param[\"parameters\"][\"max_features\"])\n",
        "        if param[\"method\"] == \"vectorisation_ponderee\" : \n",
        "            self.data.vectorisation_ponderee(ngram_range = tuple(param[\"parameters\"][\"ngram_range\"]), max_features = param[\"parameters\"][\"max_features\"], norm = param[\"parameters\"][\"norm\"])\n",
        "\n",
        "        self.etape_1_2_sauv[nom] = copy.deepcopy(self.data)\n",
        "\n",
        "    def apply_etape_3(self, param, nom):\n",
        "\n",
        "        if param == None : return \n",
        "\n",
        "        if param[\"method\"] == \"reduction_svd\" : \n",
        "            self.data.reduction_svd(n_components = param[\"parameters\"][\"n_components\"])\n",
        "        \n",
        "        if param[\"method\"] == \"reduction_nmf\" : \n",
        "            self.data.reduction_nmf(n_components = param[\"parameters\"][\"n_components\"], init = param[\"parameters\"][\"init\"])\n",
        "        \n",
        "        self.etape_1_2_3_sauv[nom + \",\" + json.dumps(param, ensure_ascii=False)] = copy.deepcopy(self.data)\n",
        "\n",
        "\n",
        "\n",
        "    def generate_data_hyperparam_combinations(self):\n",
        "        all_combinations = []\n",
        "\n",
        "        preprocessing_steps = self.hyper_data.get(\"preprocessing\", {})\n",
        "        vectorization_steps = self.hyper_data.get(\"vectorization\", {})\n",
        "        reduction_steps = self.hyper_data.get(\"dimension_reduction\", {})\n",
        "\n",
        "        # Check if all methods for an entire step are disabled\n",
        "        if not any(self.sequence_data.get(step, False) for step in preprocessing_steps):\n",
        "            preprocessing_steps = {}\n",
        "        if not any(self.sequence_data.get(step, False) for step in vectorization_steps):\n",
        "            vectorization_steps = {}\n",
        "        if not any(self.sequence_data.get(step, False) for step in reduction_steps):\n",
        "            reduction_steps = {}\n",
        "\n",
        "        preprocessing_combos = self._generate_combos_data(preprocessing_steps, \"preprocessing\")\n",
        "        vectorization_combos = self._generate_combos_data(vectorization_steps, \"vectorization\")\n",
        "        reduction_combos = self._generate_combos_data(reduction_steps, \"dimension_reduction\")\n",
        "\n",
        "        if not preprocessing_combos:\n",
        "            preprocessing_combos = [None]\n",
        "        if not vectorization_combos:\n",
        "            vectorization_combos = [None]\n",
        "        if not reduction_combos:\n",
        "            reduction_combos = [None]\n",
        "\n",
        "        for pre in preprocessing_combos:\n",
        "            for vec in vectorization_combos:\n",
        "                for red in reduction_combos:\n",
        "                    all_combinations.append({\n",
        "                        \"preprocessing\": pre,\n",
        "                        \"vectorization\": vec,\n",
        "                        \"dimension_reduction\": red\n",
        "                    })\n",
        "\n",
        "        return all_combinations\n",
        "\n",
        "    def _generate_combos_data(self, steps, step_type):\n",
        "        \"\"\"Generate all valid combinations for a specific step type.\"\"\"\n",
        "        step_combinations = []\n",
        "        for step_name, params in steps.items():\n",
        "            if self.sequence_data.get(step_name, False):\n",
        "                param_combos = self._generate_param_data_combinations(params)\n",
        "                for combo in param_combos:\n",
        "                    step_combinations.append({\"method\": step_name, \"parameters\": combo})\n",
        "        return step_combinations\n",
        "\n",
        "    def _generate_param_data_combinations(self, params):\n",
        "        \"\"\"Generate all combinations of parameters for a given step.\"\"\"\n",
        "        if not params:\n",
        "            return [{}]\n",
        "\n",
        "        keys, values = zip(*params.items())\n",
        "        param_combos = [dict(zip(keys, combination)) for combination in product(*values)]\n",
        "        return param_combos\n",
        "\n",
        "    def valider_nomenclature(self, sequence_data, sequence_classifieur):\n",
        "        \"\"\"\n",
        "        Méthode pour valider que les dictionnaires respectent la nomenclature attendue.\n",
        "        \"\"\"\n",
        "        # Validation de sequence_data\n",
        "        expected_sequence_data_keys = [\n",
        "            \"lemmatisation\", \"racinisation\", \"vectorisation_simple\", \n",
        "            \"vectorisation_ponderee\", \"reduction_svd\", \"reduction_nmf\"\n",
        "        ]\n",
        "        \n",
        "        # Vérifier si toutes les clés attendues sont présentes dans sequence_data\n",
        "        if not all(key in sequence_data for key in expected_sequence_data_keys):\n",
        "            raise ValueError(\"Erreur: Le dictionnaire 'sequence_data' doit contenir toutes les clés attendues: \" + \", \".join(expected_sequence_data_keys))\n",
        "        \n",
        "        # Vérifier que les valeurs sont bien des booléens\n",
        "        if not isinstance(sequence_data[\"lemmatisation\"], bool) or not isinstance(sequence_data[\"racinisation\"], bool) or \\\n",
        "           not isinstance(sequence_data[\"vectorisation_simple\"], bool) or not isinstance(sequence_data[\"vectorisation_ponderee\"], bool) or \\\n",
        "           not isinstance(sequence_data[\"reduction_svd\"], bool) or not isinstance(sequence_data[\"reduction_nmf\"], bool):\n",
        "            raise ValueError(\"Erreur: Les valeurs dans 'sequence_data' doivent être de type booléen.\")\n",
        "        \n",
        "        # Validation de sequence_classifieur\n",
        "        expected_classifiers = [\n",
        "            Multinomial, Bernoulli, Gaussian, RegressionLogistique,\n",
        "            KPlusProchesVoisins, ArbreDeDecision, ForetAleatoire\n",
        "        ]\n",
        "        \n",
        "        # Vérifier si toutes les clés attendues sont présentes dans sequence_classifieur\n",
        "        if not all(key in sequence_classifieur for key in expected_classifiers):\n",
        "            raise ValueError(\"Erreur: Le dictionnaire 'sequence_classifieur' doit contenir toutes les clés attendues: soit les 7 classifieurs \")\n",
        "        \n",
        "        # Vérifier que les valeurs sont bien des booléens\n",
        "        if not isinstance(sequence_classifieur[Multinomial], bool) or not isinstance(sequence_classifieur[Bernoulli], bool) or \\\n",
        "           not isinstance(sequence_classifieur[Gaussian], bool) or not isinstance(sequence_classifieur[RegressionLogistique], bool) or \\\n",
        "           not isinstance(sequence_classifieur[KPlusProchesVoisins], bool) or not isinstance(sequence_classifieur[ArbreDeDecision], bool) or \\\n",
        "           not isinstance(sequence_classifieur[ForetAleatoire], bool):\n",
        "            raise ValueError(\"Erreur: Les valeurs dans 'sequence_classifieur' doivent être de type booléen.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "72\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[111], line 52\u001b[0m\n\u001b[0;32m     39\u001b[0m json_resultat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_resultat.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m###########################################################################################################################################################################\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#########################################################                   FIN DES DEFINITIONS                ############################################################\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m###########################################################################################################################################################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m###########################################################################################################################################################################\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m###########################################################################################################################################################################\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m projet \u001b[38;5;241m=\u001b[39m \u001b[43mProjet\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msequence_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_classifieur\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msequence_classifieur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_json\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlimit_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_resultat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjson_resultat\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[110], line 38\u001b[0m, in \u001b[0;36mProjet.__init__\u001b[1;34m(self, sequence_data, sequence_classifieur, data_json, limit_data, json_resultat)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39metape_1_2_3_sauv \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_res \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[110], line 67\u001b[0m, in \u001b[0;36mProjet.main_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_etape_3(combinaison[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension_reduction\u001b[39m\u001b[38;5;124m\"\u001b[39m], etape_1_2_3_str)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m etape_1_2_str \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39metape_1_2_sauv \u001b[38;5;129;01mand\u001b[39;00m preprocessing_str \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39metape_1_sauv \u001b[38;5;129;01mand\u001b[39;00m etape_1_2_3_str \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39metape_1_2_3_sauv:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_etape_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombinaison\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessing_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_etape_2(combinaison[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectorization\u001b[39m\u001b[38;5;124m\"\u001b[39m], etape_1_2_str)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_etape_3(combinaison[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension_reduction\u001b[39m\u001b[38;5;124m\"\u001b[39m], etape_1_2_3_str)\n",
            "Cell \u001b[1;32mIn[110], line 129\u001b[0m, in \u001b[0;36mProjet.apply_etape_1\u001b[1;34m(self, param, nom)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m : \u001b[38;5;28;01mreturn\u001b[39;00m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mracinisation\u001b[39m\u001b[38;5;124m\"\u001b[39m : \n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mracinisation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_stopwords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore_stopwords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatisation\u001b[39m\u001b[38;5;124m\"\u001b[39m : \n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mlemmatisation(mode \u001b[38;5;241m=\u001b[39m param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m], overwrite \u001b[38;5;241m=\u001b[39m param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "Cell \u001b[1;32mIn[101], line 32\u001b[0m, in \u001b[0;36mData.racinisation\u001b[1;34m(self, ignore_stopwords)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mracinisation\u001b[39m(\u001b[38;5;28mself\u001b[39m, ignore_stopwords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     31\u001b[0m     stemmer \u001b[38;5;241m=\u001b[39m SnowballStemmer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage)\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_column] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_column\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupprimer_stopwords()\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
            "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
            "Cell \u001b[1;32mIn[101], line 32\u001b[0m, in \u001b[0;36mData.racinisation.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mracinisation\u001b[39m(\u001b[38;5;28mself\u001b[39m, ignore_stopwords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     31\u001b[0m     stemmer \u001b[38;5;241m=\u001b[39m SnowballStemmer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage)\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_column] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_column]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(x)\u001b[38;5;241m.\u001b[39msplit()]))\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupprimer_stopwords()\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\nltk\\stem\\snowball.py:2460\u001b[0m, in \u001b[0;36mFrenchStemmer.stem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m   2458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step1_success \u001b[38;5;129;01mor\u001b[39;00m rv_ending_found:\n\u001b[0;32m   2459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m suffix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step2a_suffixes:\n\u001b[1;32m-> 2460\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mendswith(suffix):\n\u001b[0;32m   2461\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2462\u001b[0m                 suffix \u001b[38;5;129;01min\u001b[39;00m rv\n\u001b[0;32m   2463\u001b[0m                 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rv) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(suffix)\n\u001b[0;32m   2464\u001b[0m                 \u001b[38;5;129;01mand\u001b[39;00m rv[rv\u001b[38;5;241m.\u001b[39mrindex(suffix) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__vowels\n\u001b[0;32m   2465\u001b[0m             ):\n\u001b[0;32m   2466\u001b[0m                 word \u001b[38;5;241m=\u001b[39m word[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(suffix)]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "###########################################################################################################################################################################\n",
        "###########################################################################################################################################################################\n",
        "#########################################################               DEFINITION DES PARAMETRES                ##########################################################\n",
        "###########################################################################################################################################################################\n",
        "###########################################################################################################################################################################\n",
        "\n",
        "###### CHANGER LES PARAMETRES POUR LES SEQUENCE CI-DESSOUS ######\n",
        "\n",
        "## POUR LES DATA ##\n",
        "#si les deux méthode d'une étape sont False (ex : lemmatisation et racinisation pour l'étape preprocessing )\n",
        "#Alors l'étape serat toujours égale à null\n",
        "sequence_data = {\n",
        "    \"lemmatisation\": True,\n",
        "    \"racinisation\": True,                                      #########################################################\n",
        "    \"vectorisation_simple\": False,                              ### Attention, une des vectorisation doit être cochée ###\n",
        "    \"vectorisation_ponderee\": True,                             ###  Vectorisation simple se mettra par defaut sinom  ###\n",
        "    \"reduction_svd\": False,                                     #########################################################\n",
        "    \"reduction_nmf\": False\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "## POUR LES CLASSIFIEURS ##\n",
        "sequence_classifieur = {\n",
        "    Multinomial : False,\n",
        "    Bernoulli : True,\n",
        "    Gaussian : True,                       \n",
        "    RegressionLogistique : False,\n",
        "    KPlusProchesVoisins : False,\n",
        "    ArbreDeDecision : False,\n",
        "    ForetAleatoire : False\n",
        "}\n",
        "\n",
        "###### CHANGER SOURCE DES DONNEES ######\n",
        "data_json = \"train.jsonl\"\n",
        "limit_data = 5000\n",
        "\n",
        "\n",
        "json_resultat = \"json_resultat.json\"\n",
        "\n",
        "###########################################################################################################################################################################\n",
        "#########################################################                   FIN DES DEFINITIONS                ############################################################\n",
        "###########################################################################################################################################################################\n",
        "\n",
        "\n",
        "###########################################################################################################################################################################\n",
        "###########################################################################################################################################################################\n",
        "#########################################################               lANCEMENT SCRIPT PRINCIPAL                #########################################################\n",
        "###########################################################################################################################################################################\n",
        "###########################################################################################################################################################################\n",
        "\n",
        "projet = Projet(sequence_data = sequence_data, sequence_classifieur = sequence_classifieur, data_json = data_json, limit_data = limit_data, json_resultat = json_resultat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exportation Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[112], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m500\u001b[39m],\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqrt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m      7\u001b[0m }\n\u001b[0;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m Data(test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatisation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m data\u001b[38;5;241m.\u001b[39mvectorisation_ponderee()\n\u001b[0;32m     12\u001b[0m data\u001b[38;5;241m.\u001b[39mreduction_nmf(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
            "Cell \u001b[1;32mIn[101], line 42\u001b[0m, in \u001b[0;36mData.lemmatisation\u001b[1;34m(self, mode, overwrite)\u001b[0m\n\u001b[0;32m     40\u001b[0m     lemmatizer\u001b[38;5;241m.\u001b[39moverwrite \u001b[38;5;241m=\u001b[39m overwrite\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_column] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_column]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(x)\u001b[38;5;241m.\u001b[39msplit()]))\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupprimer_stopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[101], line 25\u001b[0m, in \u001b[0;36mData.supprimer_stopwords\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words])\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Appliquer la fonction de nettoyage à la colonne texte\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_column] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_column\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnettoyer_texte\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Brasi\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
            "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
            "Cell \u001b[1;32mIn[101], line 22\u001b[0m, in \u001b[0;36mData.supprimer_stopwords.<locals>.nettoyer_texte\u001b[1;34m(texte)\u001b[0m\n\u001b[0;32m     20\u001b[0m texte \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, texte)      \u001b[38;5;66;03m# Retirer les chiffres\u001b[39;00m\n\u001b[0;32m     21\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(texte\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words])\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "data = Data(test=True)\n",
        "data.lemmatisation()\n",
        "data.vectorisation_ponderee()\n",
        "data.reduction_nmf(n_components=30)\n",
        "data_test=data.get_data()\n",
        "print(\"data_tester terminé\")\n",
        "\n",
        "data = Data()\n",
        "data.lemmatisation()\n",
        "data.vectorisation_ponderee()\n",
        "data.reduction_nmf(n_components=30)\n",
        "data_train=data.get_data()\n",
        "print(\"data_entainement terminer\")\n",
        "\n",
        "foret = ForetAleatoire(data_train)\n",
        "foret.recherche_hyperparametres(param_grid)\n",
        "print(\"hyperparametre_termine\")\n",
        "print(foret.f1_score())\n",
        "fichier=foret.predict(data_test)\n",
        "print(\"prediction terminer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ANALYSE "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Analyse():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.dossier = \"Résultat\"\n",
        "\n",
        "        self.charger_total()  #self.total\n",
        "\n",
        "        self.nb_individus = len(self.total)\n",
        "\n",
        "    def charger_total(self):\n",
        "        self.total = []\n",
        "        for fichier in os.listdir(self.dossier):\n",
        "            chemin_fichier = os.path.join(self.dossier, fichier)\n",
        "            \n",
        "            if os.path.isfile(chemin_fichier):\n",
        "                with open(chemin_fichier, 'r', encoding='utf-8') as f:\n",
        "                    try:\n",
        "                        data = json.load(f)\n",
        "                        if isinstance(data, list) and all(isinstance(item, dict) for item in data):\n",
        "                            self.total.extend(data)\n",
        "                        else:\n",
        "                            print(f\"Avertissement : {fichier} ne contient pas une liste de dictionnaires.\")\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        print(f\"Erreur lors de la lecture de {fichier} : {e}\")\n",
        "        \n",
        "\n",
        "analyse = Analyse()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Matrice de confusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = Data()\n",
        "data.lemmatisation()\n",
        "data.vectorisation_ponderee()\n",
        "data.reduction_nmf(n_components=30)\n",
        "data_train=data.get_data()\n",
        "\n",
        "foret = ForetAleatoire(data_train)\n",
        "foret.entrainement()\n",
        "print(foret.f1_score(graph=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour évaluer la performance du modèle, nous avons choisi de construire une matrice de confusion. Le classifieur utilisé est la forêt aléatoire, car il a montré de bons résultats dans des scénarios similaires.\n",
        "\n",
        "En observant la matrice de confusion, nous constatons que le modèle réussit à prédire la classe  0  (annonces acceptées), mais peine à identifier correctement la classe  1  (annonces rejetées). Cela suggère que le modèle rejette trop peu d'annonces.\n",
        "\n",
        "Nous pensons que cela est dû à un prétraitement insuffisant des données, qui ne fournit pas des critères suffisamment discriminants pour bien identifier les annonces à rejeter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyse thématique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = Data()\n",
        "data.lemmatisation()\n",
        "data.vectorisation_simple()\n",
        "\n",
        "data_thematique = data.get_data()\n",
        "data_thematique=data_thematique[data_thematique.max().sort_values(ascending=True).index].iloc[:, -15:]\n",
        "data_thematique[\"cal_réponse_signalement\" ] = data.get_data()[ \"cal_réponse_signalement\" ]\n",
        "\n",
        "tableau_double_entree = data_thematique.groupby( \"cal_réponse_signalement\" ).sum()\n",
        "\n",
        "# Créer la heatmap\n",
        "plt.figure(figsize=(12, 8))  # Taille de l'image\n",
        "sns.heatmap(tableau_double_entree, annot=True, cmap= \"YlGnBu\" , fmt= \"d\" , cbar=True)\n",
        "\n",
        "# Ajouter des titres et labels pour la heatmap\n",
        "plt.title( \"Matrice de fréquence des mots selon la modalité de cal_réponse_signalement\" )\n",
        "plt.xlabel( \"Mots\")\n",
        "plt.ylabel( \"Modalités de cal_réponse_signalement\" )\n",
        "\n",
        "# Afficher la heatmap\n",
        "plt.show()\n",
        "\n",
        "tableau_normalise = tableau_double_entree.div(tableau_double_entree.sum(axis=0), axis=1) * 100\n",
        "\n",
        "# Créer le graphique à barres horizontales 100% pour chaque mot\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = tableau_normalise.T.plot(kind='barh', stacked=True, width=0.8, color=sns.color_palette( \"Set2\", len(tableau_normalise)), legend=False)\n",
        "\n",
        "plt.title( \"Répartition des mots par modalités de cal_réponse_signalement (%)\" )\n",
        "plt.xlabel( \"Pourcentage (%)\" )\n",
        "plt.ylabel( \"Mots\" )\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, labels=[f'{v:.1f}%' for v in container.datavalues], label_type= \"center\" )\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Au début du projet, nous avons supposé que le pré-traitement des données devait être amélioré pour mieux distinguer les annonces gardées et rejetées. Initialement, nous avons basé notre analyse sur l'hypothèse que l'annonce elle-même pouvait définir la classe. Nous avons donc choisi de ne prendre en compte que la colonne texte_annonce, en négligeant les autres variables. Après avoir effectué une lemmatisation et une vectorisation simple, nous avons généré un tableau à double entrée pour observer la fréquence des mots selon les deux modalités : annonce gardée et annonce rejetée.\n",
        "\n",
        "Nous constatons que le mot le plus fréquent dans les annonces est  marché , un terme qui apparaît particulièrement souvent dans les annonces gardées. De plus, la plupart des mots les plus fréquents dans le tableau sont également ceux qui sont prédominants dans les annonces gardées. Cela renforce notre hypothèse initiale selon laquelle le pré-traitement des données ne fournit pas des valeurs discriminantes suffisantes pour bien différencier les deux classes (annonces gardées et annonces rejetées).\n",
        "\n",
        "Ensuite, nous avons tracé un graphique pour analyser la répartition des mots par modalité. Les résultats montrent que les mots les plus fréquents sont dominés par ceux des annonces gardées, ce qui suggère que le modèle rencontre des difficultés à détecter les caractéristiques spécifiques aux annonces rejetées."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
